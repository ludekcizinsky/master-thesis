%% ----------------------------------------------------------------------------
% CVG SA/MA thesis template
%
% Created 03/08/2024 by Tobias Fischer
%% ----------------------------------------------------------------------------

% -------- Instructions for writing the discussion section:
% The discussion section gives an interpretation of what you have done \cite{day2006wap}:

% \begin{itemize}
 % \item \textit{What do your results mean?} Here you discuss, but you do not recapitulate results. Describe principles, relationships and generalizations shown. Also, mention inconsistencies or exceptions you found.
 % \item \textit{How do your results relate to other's work?} Show how your work agrees or disagrees with other's work. You can rely on the information you presented in the ``related work'' section.
 % \item \textit{What are the implications and applications of your work?} State how your methods may be applied and what implications might be. 
% \end{itemize}

% \noindent Make sure that the introduction/related work and the discussion section act as a pair, i.e. ``be sure the discussion section answers what the introduction section asked'' \cite{day2006wap}. 


% todos:
% - we also address the use of smplx instead of smpl limitation mentioned in multiply, however we have no metrics nor figures to back this up



\newpage
\chapter{Discussion}

\paragraph{High-level takeaways.}
Our method produces plausible renderings with novel view synthesis quality that is broadly on par with MultiPly (Table~\ref{tab:nvs_results_hi4d}). At the same time, we observe clear limitations in pose accuracy and mesh reconstruction quality (Table~\ref{tab:pose_results_hi4d} and Table~\ref{tab:reconstruction_results}). We attribute the reconstruction gap in part to the fact that our 3DGS representation is optimized for rendering rather than for surface extraction, and our meshes are obtained via TSDF fusion of rendered depth maps. This makes geometry sensitive to residual pose errors and to the details of the extraction procedure.

\paragraph{Composability of explicit representations.}
Beyond runtime, an advantage of our explicit 3DGS representation is composability. Since Gaussians can be rendered and optimized in a modular way, our dynamic human representation can be naturally combined with explicit methods that focus on static scene reconstruction, for example by jointly rendering a static background model together with the posed humans. In contrast, implicit scene representations such as signed distance fields often require specialized rendering and integration schemes, which can make composition with external components less straightforward.

\paragraph{Efficiency and practicality.}
A practical advantage of our pipeline is efficiency. On a single NVIDIA V100 GPU, our training stage takes on the order of tens of minutes per scene and depends on the number of frames, as summarized in Table~\ref{tab:training_speed_breakdown}. Preprocessing adds additional overhead and is typically on the order of 10 to 15 minutes per scene in our implementation. In contrast, MultiPly reports optimization times on the order of one day per person and notes that its runtime scales approximately linearly with the number of people because it optimizes an implicit signed distance field representation separately for each person \cite{multiply}. MultiPly reports runtimes on NVIDIA A100 GPUs, so the corresponding wall-clock times can be expected to increase on slower, more widely available hardware such as V100-class GPUs. Overall, this makes our approach substantially faster and more practical for iteration, even though it remains a multi-stage system with non-trivial dependencies and is less convenient than feed-forward methods that directly predict outputs from the input video.


\paragraph{Role of pose.}
We model motion through skinning weights and SMPL-X pose parameters. We chose this formulation over template-free approaches such as Shape-of-Motion (SOM) \cite{som} because it leverages strong human body priors from the parametric model. This is particularly important in monocular settings where long-term point tracking can be unreliable. We initialize poses with Human3R and apply a short pose-tuning stage during training. However, the overall system remains sensitive to pose quality. Incorrect pose places Gaussians at incorrect image locations and provides an inconsistent optimization signal for the canonical 3DGS. This is consistent with Table~\ref{tab:pose_results_hi4d}. Compared to MultiPly, MPJPE remains the main gap, while we are competitive on vertex-space and interaction metrics. A practical next step is to add pose refinement guided by 2D keypoints, for example ViTPose, and to use a more robust pose estimator for challenging interactions and occlusions, for example PromptHMR \cite{wang2025prompthmr}.

\paragraph{Novel view synthesis.}
The novel view synthesis results (Table~\ref{tab:nvs_results_hi4d}) show that our method preserves global structure well and achieves the best SSIM. The remaining difference to MultiPly is mainly PSNR, while LPIPS is very close. This suggests that the gap is dominated by pixel-level discrepancies rather than a clear difference in perceptual or structural quality. Improving geometry and pose consistency is a promising direction for improving pixel-level fidelity while retaining strong structural similarity.


\paragraph{Role of DiFix.}
Qualitatively, DiFix improves local appearance and reduces low-level rendering artifacts, but it does not reliably correct high-level geometric errors such as inaccurate silhouettes or missing limbs. This matches its role as an image refinement model rather than a geometry estimator. One possible direction is to explore refinement strategies that are better aligned with human-centric failure modes, either by adapting the refinement model or by changing the supervision so that silhouette and depth consistency are emphasized. Our current view synthesis procedure uses a fixed heuristic for selecting reference views at the same timestamp and often uses the previously synthesized view in the traversal order. This can be suboptimal when key regions, such as the face, are not visible in the reference. Improving reference selection based on visibility or confidence signals is a concrete avenue to make the pseudo ground-truth supervision more reliable.

\paragraph{LHM initialization.}
The canonical initialization from LHM strongly influences downstream training because it provides the starting point for canonical Gaussians and also affects the quality of DiFix-refined supervision. A practical improvement is to use multiple candidate frames for initialization and select those with better visibility of informative regions (for example, the face or hands), and to explore ways to merge multiple LHM predictions into a more robust canonical representation. Better initialization would reduce the burden on refinement and could improve both reconstruction (Table~\ref{tab:reconstruction_results}) and view synthesis (Table~\ref{tab:nvs_results_hi4d}).

\paragraph{Towards feed-forward models.}
Feed-forward models that directly infer scene representations from video are the most attractive long-term direction, but training them typically requires large-scale, high-quality supervision. In practice, such supervision is difficult to obtain from real-world data for multi-person dynamic scenes, and simulation can be limited in visual diversity and realism. In this context, an efficient pipeline like ours can act as a supervision generator. It can produce pose, masks, and renderable scene representations at a practical runtime. This could enable scaling to larger datasets and provide training targets for future feed-forward approaches.


\section{Limitations}
\paragraph{Multi-stage preprocessing and brittleness.}
Our pipeline relies on several pretrained components (Human3R for cameras and SMPL-X, SAM3 for instance masks, Depth Anything 3 for depth maps and LHM for canonical initialization). Failures in any of these stages can propagate to training and are not always recoverable by subsequent optimization. Common failure modes include identity swaps in crowded interactions, missed or fragmented masks under occlusions, and incorrect poses under severe self-contact. As a result, the method currently benefits from manual inspection of preprocessing outputs before launching training.


\paragraph{Mesh extraction from 3DGS.}
Our representation is an explicit 3DGS optimized for rendering rather than for producing watertight surfaces. In our pipeline, meshes are extracted via TSDF fusion from per-person depth maps rendered from the 3DGS. This remains an approximation and can be sensitive to depth noise, thresholding choices, and postprocessing. This limits the reliability of geometry metrics and partially explains the gap to methods that directly optimize an implicit surface representation.

\paragraph{Input assumptions and scene scope.}
We assume RGB-only input without severe motion blur and without persistent occlusions by large objects. We focus on reconstructing dynamic humans and do not aim to reconstruct dynamic non-human objects, such as handheld props. In such cases, appearance and geometry can be absorbed incorrectly into the human model or omitted entirely. Our experiments cover short sequences (up to about 10 seconds), and we treat longer videos as outside the validated scope of this work.

\paragraph{Runtime and usability.}
Although the end-to-end runtime is on the order of minutes rather than days, the method is still non-trivial to run. It depends on multiple external models and environments, and the overall system is more suitable for advanced users than fully feed-forward alternatives. Improving robustness, packaging, and automation of preprocessing checks are necessary steps for broader usability.
