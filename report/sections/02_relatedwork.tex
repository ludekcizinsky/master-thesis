%% ----------------------------------------------------------------------------
% CVG SA/MA thesis template
%
% Created 03/08/2024 by Tobias Fischer
%% ----------------------------------------------------------------------------

% -------- Instructions for writing the related work section:
% The related work section has the following purposes: 

% \begin{itemize}
 % \item \textit{Is the overview concise?} Give an overview of the most relevant work to the needed extent. Make sure the reader can understand your work without referring to other literature.
 % \item \textit{Does the compilation of work help to define the ``niche'' you are working in?} Another purpose of this section is to lay the groundwork for showing that you did significant work. The selection and presentation of the related work should enable you to name the implications, differences, and similarities sufficiently in the discussion section.
% \end{itemize}


\newpage
\chapter{Related Work}
% ---------- Notes section

% mayne i should write about the evolution of body models - since in 25, there have been new body models from naver labs as well as meta. the reason i am using smplx is because i can then estimate pose metrics more easily, not sure if i would be able to compare to smplx if i use meta's body model

% I think another important topic crucial for realistic 4d reconstruction modelling are video diffusion models, so it may be good to somehow integrate this part into the thesis as well

% One of the main themes we have also been seeing in the last year or so is the transition from 
% per scene optimisation method to feedforward methods that can reconstruct the scene in a single forward pass.

% Another trend I need to capture is the transition from implicit representations to explicit representations such as 3dgs 

% Multi view dynamic scene reconstruction:
% 1. Free Time GS has shown that under the assumption we have access to a multi-view video of a dynamic scene with complex motions, we can reconstruct high-quality 4D representation of the scene in order of hours to days of training time. However, the main limitation of this approach is that it requires multi-view video capture, which is not always feasible. Therefore, then one might argue is that the problem boilds down to coming up with a video diffusion model that can based on the monocular video input generate novel view videos.


% Examples of state of the art method for lifting monocular video to multi view videos:
% 1. Generative Camera Dolly (GCD) - trained on synthetic data only, if I am not mistaken, can only synthesise one novel video at a time. Also I did test it myself, and the quality of the model is not great.
% 2. Cat4D - this is one of the earliest works. First of all, the model and the data is closed source which signifficantly limits any possible future research since it requites quite signifficant amount of resources to train. If I remember correctly, one of the limitations of this approach is A) it is mostly trained on synthetic data, and B) they show very limited novel view deviation - and this is where it indeed gets very difficult to hallucinate views which are on t he complete other side of the person for instance.
% 3. SV4D - this is I would say open source version of Cat4d. The main problem with this method is that they assume: single object scenes with ideally no background and simple motion. So these are quite limiting constraints.
% --- Also all these methods assume as input input from a **static** camera, which is quite limiting for real world applications.

% Why do we even need explicit scene representation - can we just use diffusion models?
% - I should try comment on this as well based on the 3dv talk from J.Barron

% Notes on implicit formats:
% - I think that it is important to mention NeRF paper which started this whole wave of implicit neural representations for 3D reconstruction. NeRF back then had two main limitatioshn which was the need for multi view inputs and also really slow training time.
% - This sparked a new direction of papers that addressed these limitations espeically the training time, but still one of the problems with impliciti representation is it is quite hard to compose them where as with 3dgs you can easily train separate gaussians for different objects in the scene and then just combine them together. This also has to do with editability - you can easily move around gaussians in 3d space, whereas with implicit representations this is not so straightforward.
% - Also one of the main issues with NeRF is still its rendering speed - ultimately, i thing in the last two years, we have seen transitiopn from implicit representations to explicit representations and various forms of 3DGS
% - One thing where however implict representations still shines is its generalisation in sparse view settings. By defintin, the implciti model needs to learn a continuous function that maps from 3d space to color and density, so it can interpolate between the known views quite well. On the other hand, explicit representations such as 3dgs need to store all the information in the gaussians themselves, so if there is not enough gaussians to cover the space, it might lead to holes in the reconstruction or just weird artifacts, hence having dense set of views is crucial for good quality reconstruction.

% Notes on dynamic explicit scene representations formats
% - 1. Deformation field over canonical representation: have a canonical space and then then track deformations over time. Of course then the question becomes how to track this motion over time. For instance:
% -- a. use point tracking and then use these points to guide the deformation field 
% -- b. parametric human models (SMPL, SMPL-X, etc.) - however these are limited to humans only
% -- I think it is important to note here that canonical representation's pro is that it can be animated from any motion that we can recover, whereas explicit time varying representation is limited to the motion seen during training only
% - 2. Explicit time parametrisation - add time as an additional input to the representation. 
% - 3. Topology chaning representations - e.g. free time gs where gaussians can appear and disappear over time.
% I think the important thing to note here is that choice of different representtion formats then also influences how difficult is to for instance obtain mesh extraction, or how well the representation can handle topology changes.

% Notes on models that I have actually tried:
% 1. Generative camera dolly - I think this one of the first papers to introduce explicit camera control for the novel view synthesis, however, the main limitation of their method was: 1. scope of data - very limited domain - trained on synthetic data - multiple objects 2. the farther from the original you go, the worse quality. When I tried their model on their own data, it did not work so well, and it did not work essentially at all on my football data
% 2. Guess the unseen - the main limitation of this model is that they have to tune single SD1.5 for every person in the scene. And from what I have observed the model in addition to that hallucinated a lot - and I was only able to visualise the original view of the scene. Another issue with guess the unseen was their separation of dynamic and static background - their masking pipeline often failed and as a result you could see how static background is actually attached to the dynamic objects.
% 3. Shape of motion - you can see with their demo on the website that the nvs from extreme viewpoints sucks
% 4. LHM - this works pretty good in order to get initial results, but it only reasons about the canonical apperance and geometry. You have to separately model the motion, and this becomes an issue with state of the art pose estimatros - as of now. In addition, they still assume quite clean capture of the person to infer the canonical representation - therefore it is neccesary to be smart about how you choose the frames


% Notes on tempalate based vs tracking methods for modelling motion
% - Template based methods - e.g. SMPL, SMPL-X, etc. These methods are great because they provide a strong prior on human shape and motion, which helps to regularise the reconstruction. However, these methods are limited to the expressiveness of the underlying model, and often fail to capture clothing dynamics and other non-rigid deformations.

% - Tracking based methods - these methods do not rely on a predefined template, and instead track points or features over time to capture motion. These methods are more flexible and can capture a wider range of motions and deformations, but they are also more prone to drift and errors over time. Also for a monocular setting, as they argue in MVTracker, these methods often require multi-view input to obtain reliable tracking.

% I think another important aspect of my work is to highlight the differene between rasterisaton and ray tracing

% ---------- end of notes section

% keep in mind the following writing rules:
% 1. What problem dimension does this subsection cover, and why does it matter for my paper?
% e.g. “This subsection reviews methods for monocular human reconstruction under occlusion, because occlusion is the main failure mode in our setting.”

% 2. Start with a functional opening paragraph (not a polished one)
% e.g. “A large body of work has studied [problem setting]. Existing approaches can be broadly grouped into [2–3 categories], which differ mainly in [key axis: representation, supervision, assumptions, etc.].”

% 3. Write in “paper clusters,” not paper-by-paper
% - Group papers before writing
% - Write one sentence per group, not per paper

% 4. End each subsection with a bridge sentence
%  “What do all these approaches still fail to address that motivates the next section or my method?”

% ---------

% 5 min – Write the scope sentence (“This subsection reviews…”)

% 10 min – Write 3–5 bullet points describing clusters of work

% 10 min – Turn bullets into rough prose (no citations yet)

% 5 min – Add citations and a closing bridge sentence

\section{Parametric Body Models}
% notes:
% also called statistical body models

% scope sentence: in our work, we rely on parametrric body model to a. anchor our explicit 3d representation and b. to define the deformation field over time.
% clusters:
% - 1. early body models before smpl: 
% issues:
% - traditional approach: model how vertices are related to the underlying skeleton via skinning weights -> while these approaches are supported by rendering engines, they generate unrealistic joint deformations 
% - support limited variety of body shapes
% - some are not compatible with with back then existing graphics pipeines and rendering engines

% goals of smpl: make the model as simple as possible, as well as standardised so that it can be used by the community at large, and keep the realistic defomration and shapes learned from data

% - 2. smpl and its variants (smplh, smplx)
% in general, smpl divides the body modelling into two parts: 
% a. shape model - the question is how to offset each vertex based on the shape parameters
% b. pose model - you have predefined set of joints, and the question is how to rotate each of these joints - they use recursive kinematic tree with blend skinning weights to define how each vertex is influenced by the underlying joints

% smpl stands for skinned multi-person linear model
% smplx - prior works only focus either on body only or hands, not separately - except from an early attempts
% ---Frank model - which however just stitched together seprarate hands, face and body models, this stitching result in unrealistic model
% --- SMPL-H - extends smpl by adding articulated hands however it still does not model the face -> therefore they start from smpl-h and then add face model on top of it (FLAME)
% in addition to smpl, smplx combines smpl body model with FLAME head model and MANO hand model

% both smpl and smpl are models learned from large scale dataset of 3d body scans, smplx says 5586 scans 

% FLAME in contrast to previous approaches that focus only on face models the entire head


% - 3. modern approaches - annybody, meta momentum human rig (mhr)
% mhr decouples skeletal pose from body shape to provide more control and higher interpretability -> they buiold this based on human momentum rig paper which is in turn based on ATLAS 
% annybody - argues that smpl is trained on scans with limited population diversity, and therefore struggles to generalise to for instance children or elderly people. therefore they take scan free approach and learn the body priors instead from the community driven MakeHuman framework


Modelling human bodies is a long standing and important problem in computer vision and graphics. The foundational paper in this area is the Skinned Multi-Person Linear (SMPL) model \cite{smpl}. Unlike its predecesors, SMPL was the first parametric model that was compatible with existing graphics pipelines, differentiable and capable of generaating realistic body shapes and poses since it was fitted on a large corpora of 3D scans. SMPL divides the body modelling into two parts. First, it models the shape variations via learned PCA shape space. Specifically, to obtain shape of given person, SMPL requires 10 shape parameters that define the coefficients of the PCA basis. Together, linear combination of these basis vectors define the offsets for each vertex in the body mesh. Second, the pose of the mesh is modelled via a predefined kinematic tree of joints. During inference, human mesh recovery method estimate the local rotation of each joint in the kinematic tree. The tree is then recursively travsersed to obtain the globabal transformation of each joint. Finally, to define how each vertex in the mesh is influenced by the underlying joints, SMPL uses the learned skinning weights to combine the transformations of all joints affecting given vertex.

While SMPL enabled major step towards realistic human body modelling, it still lacked the ability to model hand and face details. To address this limitation, SMPL-H extended the SMPL model by adding articulated hand modelling \cite{smplh}. Later, SMPL-X extends the SMPL-H by adding also detailed face 
modelling based on the FLAME head model \cite{smplx,flame}, and therefore marks the first method to jointly model body, hands and face in a single parametric model. Most recently, Anny \cite{annybody} introduces a new parametric body model that aims to address the limited population diversity in the training data of SMPL models. Specifically, AnnyBody learns body shape priors from the community driven MakeHuman framework, which contains a wide variety of body shapes including children and elderly people.
Finally, most recently Meta introduced the Momentum Human Rig (MHR) \cite{yang2025sam3dbody}, which decouples skeletal pose from body shape to provide more control and higher interpretability. 

We use SMPl-X body model in our work since it is being predicted by our chosen HMR method. We use the SMPL-X model to define the deformation field over time for our explicit 3D representation and also to 
provide strong geometric prior for human body shape.


\section{Human Motion Estimation}

% why this section: in order to use smpl-x body model, we need to first estimate its parmaeters
% from the monocular video
% 22 methods: glamr
% 23 methods: humans in 4d
% 24 methods: wham, tram, pace
% 25 methods: Hsfm, camera HMR, PromptHMR, GENMO, human3r, sam3d body

% local human mesh recovery: 
% - goal: estimate parameters for mesh based body model from a single image in the local camera frame
% - multiple people:
% -- you can use top down approach: first detect people via object detector, then crop each person and then run single person hmr method, e.g. Humans in 4D, camera hmr
% -- bottom up approach: first estimate all keypoints in the image, then group them into people, then run single person hmr method on each person. This should address the problems with more crowded scenes where people are close to each other. e.g. multi-hmr

% global human mesh recovery:
% - goal: estimate parameters for mesh based body model from a monocular video in the global world frame
% - by defintion, very ill constrained problem since neither the camera nor the human motion is known, and decoupling these is not trivial
% - early methods such as glamr first estimate local human mesh for each frame, then use these as input to the predictor that estimate the translation of the person in the world frame, after this, there is some joint optimisation to refine the results

% - later methods such as slahmr rely on SLAM to estimate camera poses in world frame, and then use these to guide the global human mesh recovery, later TRAM improves Slahmr by enhancing the camera pose estimates to be in meters
% - in contrast to these offline methohds, more modern methods such as TRACE or WHAM take an end to end approach and regress directly the global human mesh parameters from the monocular video and making use of memory unit to store the past information about the scene 


% human3r notes:
% - key idea: to reconstrution human motion accurately, we need to understand their surroundings as well
% x hsfm reconstructions humans and scene separately and then combines them via refinement under constraints
% in general the main advantage of h3r is unified one stop solution that is fast, easy to install and deploy in contrast to more complxe multi stage pipelines

% todo: possible also mention sam3d body here since it is from meta and it is quite recent as well


Human Mesh Recovery (HMR) methods aim to estimate the parameters of parametric body models from images or videos. These methods can be categorised into two main groups: local HMR and global HMR. Local HMR methods focus on estimating body model parameters in the local camera frame from single images, while global HMR methods aim to recover body model parameters in the global world frame from monocular videos.

\paragraph{Local HMR methods}. Majority of the local HMR methods adopts a two-stage top-down approach. First, the method usually uses an off the shelf object detector to detect people in the image. Then, for each detected person, the method crops the image and runs a single-person HMR method to estimate the body model parameters \cite{4dhumans,camerahmr}. However, the top down approach often fails in crowded scenes. Therefore, bottom-up approaches first detect all joint keypoints in the image and then group these into individual per subjects keypoints before running single-person HMR \cite{multihmr}.

\paragraph{Global HMR methods}. Earlier approaches first such as GLAMR \cite{glamr} first run local HMR for each frame, followed by a separate module that estimates the global translation of the person in the world frame. Finally, a joint optimisation is performed to refine the results. Later methods such as SLAHMR \cite{slahmr} and TRAM \cite{tram} leverage SLAM to estimate camera poses in the world frame, which are then used to guide the global HMR. More recent methods such as WHAM \cite{wham} and TRACE \cite{trace} take an end-to-end approach and directly regress global body model parameters from monocular videos, often using memory units to store past information about the scene. While these recent methods achieve already promising results, they still rely on varying number of the shelf estimators such as 2D keypoint detectors, segmentation models, and depth estimators. This dependency becomes espeically problematic in video where tracking consistency is crucial for high quality results.
Most recently, Human3R \cite{chen2025human3r} proposes a unified framework that jointly reconstructs human motion and scene geometry from monocular videos. By jointly reasoning about humans and their surroundings, Human3R achieves more accurate and consistent reconstructions compared to prior multi-stage pipelines. We therefore choose Human3R as our HMR method in this work.





% nerf notes:
    % at the time, there was a new trend to encode scene or 3d object as implicit neural representation (in practice a MLP net) - e.g. sign distance field (paper: A volumetric method for building complex models from range images)
    % however at that time, these neural encoding techniques still lacked behing traditional methods, such as meshes or voxel grids,  in terms of the fidelty of the reconstructiona
    % neural 3d shape representations review:
    % - map xyz coordinates to signed distance values (e.g. deep sdf), or occupancy values (e.g. occupancy networks) -> both these examples required 3D supervision
    % - later in the paper "Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision" they were first to show how to construct differentable rendering pipeline for implicit representations (before them some works showed this for explicit representations such as voxels or meshes), another work Scene Representation Networks learns a net that for each 3d world coordinate outoputs feature vector and rgb color
    % - the prioblem with these techniques is that they were limited to only simple shapes - in contrast nerf showed how to reconstruct complex real world scenes with high fidelity
    
    % View synthesis and image-based rendering:
    % - early techniques relied on light field sample interpolation (e.g. light field rendering, lumigraph) -> these require very very dense capture of images
    % - therefore later methods focused on differentable rendering - one of the main directions focused on optimisation of mesh representation with apperance modeled via diffuse or view dependend apperance -> these did not succeed because apparantly optimising mesh from images is quite hard problem - difficult loss landscape 
    % - therefore later works switched from meshes to volumetric representations - e.g. Deepview, local light field fusion - these approaches train deeped neural net on large number of scene datasets to predict sampled volumetric representation for novel scenes -> the main limit of these volumetric scenes is the resolution - to render higher resoliton iamges, you also need to have denser volumetric grids which quickly becomes infeasible in terms of memory consumption as well as sampling time
    % - nerf overcomes this by using implicit continuous representation that can be sampled at arbitrary resolution, requires less storage, and yileds significantly better quality results


% cat 3d notes:
    % advances in photogammetry unlocked creation of high quality 3d assets from 2d inputs (images / videos) - more precisely NeRF and 3DGS have been the two most influential representations in this area
    % both methods yield highly realistic results, however under the assumption that the input to these method is a dense capture of images  which is rare in real world scenarios
    % so this is what spared interest in 
    % key idea in cat3d paper:
    % "This way, we reformulate a difficult ill-posed reconstruction problem as a generation problem: given any number of input images, generate a collection of consistent novel observations of the 3D scene"
    % -> their solution to the problem: multi-view diffusion model that given arbitrary number of input images can generate novel view images of the scene from arbitrary camera viewpoints

    % the key question in the 3d literature at the very start was how to make use of the fact that 2D data is abundant, whereas 3D data is scarce
    % DreamFusion came up with a fundmental idea of score distlation sampling (SDS) that allows to distil knowledge from 2D diffusion models into 3D representations such as NeRF

    % while this was a major step forward, DreamFusion lacked explicit camera control. This limitation was addressed by subsequent works such Zero123 or zero-nvs that introduced explicit camera conditioning into the diffusion models for 3D generation.

    % another important aspect of 3d reconstruction methods is ensuring multi-view consistency, methods such as Image Dream or cat3d encode this by using 3D attention mechanism in their diffusion models, i believe also recon fusion is a good example here

    % finally, in contrast to synthesise views and then fit NeRF or 3DGS to these views, there has been a new line of works focusing on feedforward 3d reconstruction that maps directly the input modality to the 3d representation.
    % example of such methods in the 3d space include: lrm, dmv3d, instant3d, splatter image, gs-lrm
    % while these methods are extremelly fast, they reconstruction quality is still lagging behind the optimisation based methods


% cat 4d notes:
    % similar to 3d recosntruction, under the right cappture conditions, we can reconstruct high quality 4d representations of dynamic scenes - e.g. 4dgs, free time gs, d-nerf, dynamic 3dgs
    % while for both 3d and 4d, these capture conditions are quite limiting in real world scenarios, it is still feasible to do multi-view capture of a static scene e.g. just walk around with the smartphone. In contrast, 
    % for 4d scene reconstruction, this is a different story - you literally need multiple cameras to capture the dynamic scene from different viewpoints at the same time - so the bar is much higher here

    % the key idea behind cat4d paper is: learn 4d generative model based on the mix of synthetic and real world multi-view dynamic scene data, therefore, their model learns to transform an input monocular video
    % into multi-view video, "Specifically, our model accepts any number of input images captured at different viewpoints and times, and synthesizes as output the scene’s appearance at any specified novel viewpoints and novel times"

    % dynamic 3d scene reconstruction
    % - methods assumes multi-view video capture as input: e.g. hexplane, k-planes, dynamic 3dgs, nerfies, hyper nerf, 4dgs
    % - methods focus on monocular video input: e.g. Neural Scene Flow Fields,  Mosca, shape of motion or most recently HiMor -> they key idea behind these method is to use 3dgs or nerf as the underlying canonical static representation, and learn deformation field over time to model the motion -> the issue with these methods is that they cant reconstruct what has not been observed in the input video. This becomes especially problematic when using explicit representation such as 3dgs which becomes underoptimal in sparse view settings. of course another disadvantage of these methods is that they have to rely on other off the shelf modules 


    % 4d generation:
    % - early methods relied on score distilation sampling - e.g. DreamFusion for 3d, and text-to-4d dynamic scene generation for 4d
    % - further advances in this field were driven by relying on pretrained multi-view (e.g. zero-1-to-3, mvdream) or video diffusion models (most promiment base video diffsion is stable video diffsion model), e.g. dreamgaussians4d, 4d-fy, consistent4d 

    % most recent method drop the sds optimisation, and instead focus on generating multi-video, e.g. dimension-x, cat4d, vidid-zoo, efficient-4d, diffusion^2
    % some methods take it even further and directly train feedforward 4d reconstruction models, e.g. l4gm
    % why can not you use simple video to video diffusion models for 4d gen: no explicit multi-view consistency


    % some notes on the initial score distaltion sampling idea:
    % - the key idea of sds is as follows: render your 3d representation, and then ask diffusion model to refine it, and then backprop use the added vs predicted noise to guide the direction of the update
    % - the problem with this is that the direction of the refiment depends on the input condition - in the vanila version of the sds formulation - this would be text - this is very weakly constraining signal for 3d reconstruction
    % - another issue is that you have to keep prompting diffusion model during training which is quite computationally expensive

% geometry informed video diffusion models
% - e.g gen3c: predicted point clouds from the input image depth maps, and use these as input condition to the diffusion model
% - viewcrafter - init point cloud, render, syntheseize novel views, and repeat


\section{3D and 4D Reconstruction from Dense Input Capture}

In this section, we review prior works that focus on 3D and 4D reconstruction of a given scene under the assumption of having as input dense set of views or multi-view video capture of the scene. While these capture conditions are quite limiting (espeically for the 4D case),
they lay the foundation for the later works that focus on more practical real world capture settings with sparse input views or monocular videos, which we discuss in the next section.


\paragraph{3D reconstruction methods.} The goal of 3D reconstruction techniques is to recover the chosen scene representaton from the given set of images. Classical structure from motion pipelines such as COLMAP \cite{colmap} first map the set of images to a sparse point cloud via feature matching and triangulation, followed by bundle adjustment to refine the camera poses and 3D points. Finally, the sparse point cloud is densified via multi-view stereo techniques \cite{sfmrevisited,schoenberger2016mvs}. While COLMAP recovers accurate camera poses and point clouds, it is not capable of reconstructing continuous surface. This motivates the later neural representations such capable of reconstructing continuous 3D geometru such as DeepSDF \cite{park2019deepsdflearningcontinuoussigned} and Occupancy Networks \cite{mescheder2019occupancynetworkslearning3d}. Main limitation of DeepSDF and Occupancy Networks is that they require 3D supervision during training. Later works such as Differentiable Volumetric Rendering \cite{niemeyer2020differentiablevolumetricrenderinglearning} and Scene Representation Networks \cite{sitzmann2020scenerepresentationnetworkscontinuous} show how to learn implicit neural representations without 3D supervision by introducing differentiable rendering pipelines. However, these methods are limited to reconstructing simple shapes only. NeRF \cite{mildenhall2020nerfrepresentingscenesneural} overcomes these limitations by using implicit continuous representation that can be sampled at arbitrary resolution, requires less storage, and yields significantly better quality results. However, due to its implicit nature, NeRF is slow to train as well as render from. This motivates the later explicit representations such as 3D Gaussians (3DGS) \cite{3dgs} that address these two key limitations of NeRF - fast to train and fast to render, while still yielding high quality results. Later methods such as 2D Gaussians \cite{2dgs} and Mesh Splatting \cite{held2025meshsplattingdifferentiablerenderingopaque} focus on improving the mesh extraction quality from 3DGS. In our work, we choose 3DGS as our underlying 3D representation due to its fast training and rendering speed.



\paragraph{4D reconstruction methods.} The goal of 4D reconstruction techniques is to recover the spatio-temporal scene representation from multi-view video capture of the dynamic scene. Early works such as VolumeDeform \cite{volumedeform} and DynamicFusion \cite{dynamicfusion} rely on RGB-D data and iteratively fit the underlying 4D model. Later works focus on modelling motion of the underlying scene representation. For instance, Shape of Motion \cite{som} and MOSCA \cite{lei2024moscadynamicgaussianfusion} use 3DGS as the underlying 3D representation and learn deformation field over time to model the motion. In contrast, 4D Gaussians (4DGS) \cite{4dgs} extend 3DGS to dynamic scenes by adding time as an additional input to the representation. Free Time Gaussians \cite{freetimegs} builds on top of 4dgs and introduce topology changing representations that allow Gaussians to appear and disappear over time, enabling modelling of dynamic scenes that capture complex motion. In contrast to methods such as Shape-of-Motion or Mosca that rely on point tracking to define the deformation field, we use parametric body models to define the deformation field over time. We hypothesize that parametric body prior provides stronger regularization for human motion compared to point tracking, leading to more accurate and consistent motion estimation.


\section{3D and 4D reconstruction from Sparse Input Capture}

In our work, we focus on a more practical and challenging setting of 4D reconstruction from monocular video. Therefore, we start by reviewing prior work that focus on addressing the challenges of sparse input views for 3D reconstruction, and that lay the foundation for later works that focus on monocular video input for 4D reconstruction.



\subsection{3D methods}
\paragraph{Optimisation based approaches.} Earlier works tried to better constraint the optimisation of neural representations such as NeRF or 3DGS via geometric priors. For instance, SPARF \cite{sparf} and RegNeRF \cite{regnerf} introduce reprojection losses to enforce multi-view consistency. Other methods rely on off-the-shelf modules to estimate depth or normals from the input images to guide the reconstruction process \cite{densedepthprior,sparsenerf,fsgs,monosdf,depthsupervisednerf}. However, these methods only yield marginal improvements in denser view settings and balancing the losses becomes challenging. Therefore, most recent works focus on novel view synthesis via a generative model and then use this view to backrpropagate the signal. Methods in this category differ in the way which underlying conditional model they choose, how they condition the generative model and finally how they use the generated view to guide the 3D reconstruction. Early approaches focused on using pretrained text-to-image models such as DreamFusion \cite{dreamfusion} or ProlificDreamer \cite{prolificdreamer} to synthesise novel views via score distillation sampling. However, these methods are computationally expensive since they require querying the diffusion model during training, and the text conditioning signal is often too weak to guide the 3D reconstruction. Later works such as ReconFusion \cite{reconfusion} and ViewCrafter \cite{viewcrafter} use geometry informed video diffusion models to synthesise novel views that are consistent with the input images. Other methods such as Cat3D \cite{cat3d} and SV3D \cite{sv3d} first synthesise novel views and then fit the 3D representation to these views. Finally, methods such as FlowR \cite{flowr}, DiFix \cite{wu2025difix3d} and 3DGS Enhancer \cite{3dgsenhancer} propose a new way of using diffusion models - instead of doing noise-to-data generation like DreamFusion and other score distilation techniques do, they perform conditional source distribution to target distribution generation. In practice, this means using rendered image or set of images as input to a pretrained \textit{refiner} model that outputs refined image which is then added to the collection of the training views. In contrast DiFix3D that takes as input only a single reference image and the image to be refined, FlowR was trained to take as input N input views to refine and M reference views, which should in theory help with better multi-view consistency. In our work, we choose DiFix as our generative model since it does not add large computationl overhead to our training process and as argued before, it more effectively uses the generative model priors compare to noise-to-data generation methods. We do not use FlowR because it has not been made publicly available yet.

\paragraph{Feedforward approaches.} Finally, there has been a new line of works focusing on feedforward 3D reconstruction that maps directly the input modality to the 3D representation. Early methods such as MVSNeRF \cite{mvsnerf} and PixelNeRF \cite{pixelnerf} were the first significant approaches to tackle the generalisability of NeRF by training on large datasets of multiple scenes. Later methods focused on directly predicting 3D representation from sparse set of images - for instance, PixelSplat \cite{pixelsplat}, MV-Splat \cite{mvsplat}, MV-Splat360 \cite{mvsplat360}, and LGM \cite{lgm} predict 3DGS from sparse input views. For scene-level reconstruction that also predicts camera parameters, DUST3R \cite{dust3r} and MAST3R \cite{mast3r} take pair of images and predict camera parameters along with sparse point cloud. However, these methods are limited to only pairs of images. Later works such as VGGT \cite{vggt} show that it is possible to infer all the 3D geometry from single image.  Finally, Bolt3D \cite{bolt3d} predicts 3DGS instead of point clouds to further improve the reconstruction quality. While these feedforward methods are extremely fast, espeically their novel view synthesis quality is still lagging behind the optimisation based methods, even in the simpler static scene setting.


 
\subsection{4D methods}
\paragraph{Optimisation based approaches.} In the 4D reconstruction from monocular video, there are two main categories of methods that differ in the way they combine novel view synthesis and 4D reconstruction. First category of methods such as Cat4D \cite{cat4d} and SV4D \cite{sv4d} follow a two-stage approach - first synthesising multi-view video via generative models, and then fitting the 4D representation to these views. This separation of concerns allows to better focus on each individual task of view densification and 4D reconstruction. In contrast, methods such as BulletGen \cite{bulletgen} alternate between synthesising novel views and optimising the 4D representation. Our method follows the first category, i.e., we first use DiFix \cite{wu2025difix3d} to synthesise novel views from the monocular video, and then fit our 4D representation to these views.

\paragraph{Feedforward approaches.} In the last year, there has been a new line of works focusing on feedforward 4D reconstruction from monocular videos. Despite this problem being extremelly ill-posed, this trend reflects the fact that 3D models are getting better, and therefore can serve as powerful priors for 4D reconstruction. Early methods such as Monst3r \cite{monst3r} focus on extending DUST3R \cite{dust3r} to the dynamic setting. However, due to their formulation, they still require a 2D tracker to model motion. To address this issue, Dynamic Point Maps were introduced \cite{dynamicpointmaps}, building on the idea of view-invariant point maps from DUST3R and MAST3R \cite{mast3r} and extending them to view- and time-invariant point maps to model dynamic scenes from monocular video. This representation is complete in a sense that it can reconstruct all the components of the 4D scene including motion without requiring extra 2D tracker module. Similar formulation was later adapted by ST4RTrack \cite{st4rtrack}. Other feedforward approaches such as Align3R \cite{align3r}, CUT3R \cite{cut3r}, and PI3 \cite{pi3} recover and align depth maps but still require 2D tracker or video diffusion (in case of Geo4D \cite{geo4d}) to model motion. A truly feedforward method is Any4D \cite{any4d} that models motion via scene flow and geometry via dense point maps. While these methods provide promising future directions and apart from fast inference also provide template free method without relying (mostly) on other off-the-shelf modules, their reconstruction quality is still lagging behind the optimisation based methods. In our work, we therefore try to combine the best of both worlds - we use feedforward HMR method to estimate human motion from the monocular video, and then use this motion to guide the optimisation of our 4D representation. 

\section{Monocular Human-centric 4D Reconstruction Methods}

Previously reviewed methods did not make any specific assumption about the scene content. In contrast, human-centric 4D reconstruction methods leverage the strong prior knowledge about human body shape and motion to better constrain the reconstruction process from monocular videos.


\subsection{Single Human approaches.}

\paragraph{Video as input.}
Early methods such as HumanNeRF \cite{humannerf}, NeuMan \cite{neuman}, and Vid2Avatar \cite{vid2avatar} focus on reconstructing implicit neural representations of a \textit{single} human from monocular video. The main limitation of these methods is inherent to their implicit representation - they are slow to train and render, making them impractical for real world applications. This led to introduction of explicit representations such as Drivable 3DGS \cite{drivable3dgs}, Gauhuman \cite{gauhuman}, Gaussian Avatar \cite{gaussianavatar} and ExAvatar \cite{exavatar} (current state-of-the-art in terms of reconstruction quality). The main issue of these approaches is that they assume that the input video alone is sufficient to fully reconstruct the human, however this is rarely the case, especially for in the wild videos. MV-Performer \cite{mvperformer} tries to address this limitation by fine-tuning multi-view video diffusion model to synthesise novel videos of the human based on the monocular video. However, it still assumes that there are no other human in the scene. We address this limitation by introducing a multi-human 4D reconstruction method with generative priors.

\paragraph{Image or sparse set of images as input.} In contrast to video input, these methods focus on reconstructing human from single image or sparse set of images.  Ani-GS \cite{anigs} uses multi-view video diffusion model to synthesise novel view of the canonical human, GAS \cite{gas} first fits initial NeRF representation to lift the subject into 3D and then refine this representation via video diffusion. Most recently, LHM \cite{qiu2025lhm} proposed to instead learn a feedforward model that maps image of a person to canonical 3DGS representation of the human which can be later animated via SMPL-X model. We adopt LHM as our initialisation method for each human in the scene due to its fast inference speed and reconstruction quality. However, since LHM was designed for single human reconstruction only and it also does not predict the smpl-x pose parameters, when dealing with multiple humans, we observed that LHM initialisation often leads to inter-penetration between the humans. In addition, LHM uses only a single view to estimate the canonical representation which is often not sufficient to accurately estimate all details. For this reason, LHM initialisation is further refined during our 4D reconstruction process that jointly optimises all humans in the scene.


\subsection{Multiple Human approaches} 
The literature that focuses on multiple human 4D reconstruction from monocular video is quite limited. Guess the unseen \cite{gtu} introduces a frame work where they deal with the lack of views by finetuning a single diffusion model for each person in the scene, and then they use this model to synthesise novel views of each person in the scene. This is indeed time consuming and scales poorly with number of humans in the scene. In cotnrast, MultiPly \cite{multiply} introduces a multi-human 4D reconstruction method capable of geometry and appearance reconstruction, even dealing with complex inter-human occlusions and interactions, however its main limitation is that it relies on sign distance field representation which is slow to train and render. In our work, we address the training and rendering speed of MultiPly by using explicit 3DGS representation. However, since 3DGS is more sensitive to sparse view settings, we further introduce generative priors via DiFix \cite{wu2025difix3d} to better constrain the reconstruction process.