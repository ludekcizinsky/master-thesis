%% ----------------------------------------------------------------------------
% CVG SA/MA thesis template
%
% Created 03/08/2024 by Tobias Fischer
%% ----------------------------------------------------------------------------

% -------- Instructions for writing the related work section:
% The related work section has the following purposes: 

% \begin{itemize}
 % \item \textit{Is the overview concise?} Give an overview of the most relevant work to the needed extent. Make sure the reader can understand your work without referring to other literature.
 % \item \textit{Does the compilation of work help to define the ``niche'' you are working in?} Another purpose of this section is to lay the groundwork for showing that you did significant work. The selection and presentation of the related work should enable you to name the implications, differences, and similarities sufficiently in the discussion section.
% \end{itemize}


\newpage
\chapter{Related Work}
% ---------- Notes section

% mayne i should write about the evolution of body models - since in 25, there have been new body models from naver labs as well as meta. the reason i am using smplx is because i can then estimate pose metrics more easily, not sure if i would be able to compare to smplx if i use meta's body model

% I think another important topic crucial for realistic 4d reconstruction modelling are video diffusion models, so it may be good to somehow integrate this part into the thesis as well

% One of the main themes we have also been seeing in the last year or so is the transition from 
% per scene optimisation method to feedforward methods that can reconstruct the scene in a single forward pass.

% Another trend I need to capture is the transition from implicit representations to explicit representations such as 3dgs 

% Multi view dynamic scene reconstruction:
% 1. Free Time GS has shown that under the assumption we have access to a multi-view video of a dynamic scene with complex motions, we can reconstruct high-quality 4D representation of the scene in order of hours to days of training time. However, the main limitation of this approach is that it requires multi-view video capture, which is not always feasible. Therefore, then one might argue is that the problem boilds down to coming up with a video diffusion model that can based on the monocular video input generate novel view videos.


% Examples of state of the art method for lifting monocular video to multi view videos:
% 1. Generative Camera Dolly (GCD) - trained on synthetic data only, if I am not mistaken, can only synthesise one novel video at a time. Also I did test it myself, and the quality of the model is not great.
% 2. Cat4D - this is one of the earliest works. First of all, the model and the data is closed source which signifficantly limits any possible future research since it requites quite signifficant amount of resources to train. If I remember correctly, one of the limitations of this approach is A) it is mostly trained on synthetic data, and B) they show very limited novel view deviation - and this is where it indeed gets very difficult to hallucinate views which are on t he complete other side of the person for instance.
% 3. SV4D - this is I would say open source version of Cat4d. The main problem with this method is that they assume: single object scenes with ideally no background and simple motion. So these are quite limiting constraints.
% --- Also all these methods assume as input input from a **static** camera, which is quite limiting for real world applications.

% Why do we even need explicit scene representation - can we just use diffusion models?
% - I should try comment on this as well based on the 3dv talk from J.Barron

% Notes on implicit formats:
% - I think that it is important to mention NeRF paper which started this whole wave of implicit neural representations for 3D reconstruction. NeRF back then had two main limitatioshn which was the need for multi view inputs and also really slow training time.
% - This sparked a new direction of papers that addressed these limitations espeically the training time, but still one of the problems with impliciti representation is it is quite hard to compose them where as with 3dgs you can easily train separate gaussians for different objects in the scene and then just combine them together. This also has to do with editability - you can easily move around gaussians in 3d space, whereas with implicit representations this is not so straightforward.
% - Also one of the main issues with NeRF is still its rendering speed - ultimately, i thing in the last two years, we have seen transitiopn from implicit representations to explicit representations and various forms of 3DGS
% - One thing where however implict representations still shines is its generalisation in sparse view settings. By defintin, the implciti model needs to learn a continuous function that maps from 3d space to color and density, so it can interpolate between the known views quite well. On the other hand, explicit representations such as 3dgs need to store all the information in the gaussians themselves, so if there is not enough gaussians to cover the space, it might lead to holes in the reconstruction or just weird artifacts, hence having dense set of views is crucial for good quality reconstruction.

% Notes on dynamic explicit scene representations formats
% - 1. Deformation field over canonical representation: have a canonical space and then then track deformations over time. Of course then the question becomes how to track this motion over time. For instance:
% -- a. use point tracking and then use these points to guide the deformation field 
% -- b. parametric human models (SMPL, SMPL-X, etc.) - however these are limited to humans only
% -- I think it is important to note here that canonical representation's pro is that it can be animated from any motion that we can recover, whereas explicit time varying representation is limited to the motion seen during training only
% - 2. Explicit time parametrisation - add time as an additional input to the representation. 
% - 3. Topology chaning representations - e.g. free time gs where gaussians can appear and disappear over time.
% I think the important thing to note here is that choice of different representtion formats then also influences how difficult is to for instance obtain mesh extraction, or how well the representation can handle topology changes.

% Notes on models that I have actually tried:
% 1. Generative camera dolly - I think this one of the first papers to introduce explicit camera control for the novel view synthesis, however, the main limitation of their method was: 1. scope of data - very limited domain - trained on synthetic data - multiple objects 2. the farther from the original you go, the worse quality. When I tried their model on their own data, it did not work so well, and it did not work essentially at all on my football data
% 2. Guess the unseen - the main limitation of this model is that they have to tune single SD1.5 for every person in the scene. And from what I have observed the model in addition to that hallucinated a lot - and I was only able to visualise the original view of the scene. Another issue with guess the unseen was their separation of dynamic and static background - their masking pipeline often failed and as a result you could see how static background is actually attached to the dynamic objects.
% 3. Shape of motion - you can see with their demo on the website that the nvs from extreme viewpoints sucks
% 4. LHM - this works pretty good in order to get initial results, but it only reasons about the canonical apperance and geometry. You have to separately model the motion, and this becomes an issue with state of the art pose estimatros - as of now. In addition, they still assume quite clean capture of the person to infer the canonical representation - therefore it is neccesary to be smart about how you choose the frames


% Notes on tempalate based vs tracking methods for modelling motion
% - Template based methods - e.g. SMPL, SMPL-X, etc. These methods are great because they provide a strong prior on human shape and motion, which helps to regularise the reconstruction. However, these methods are limited to the expressiveness of the underlying model, and often fail to capture clothing dynamics and other non-rigid deformations.

% - Tracking based methods - these methods do not rely on a predefined template, and instead track points or features over time to capture motion. These methods are more flexible and can capture a wider range of motions and deformations, but they are also more prone to drift and errors over time. Also for a monocular setting, as they argue in MVTracker, these methods often require multi-view input to obtain reliable tracking.

% I think another important aspect of my work is to highlight the differene between rasterisaton and ray tracing

% ---------- end of notes section

% keep in mind the following writing rules:
% 1. What problem dimension does this subsection cover, and why does it matter for my paper?
% e.g. “This subsection reviews methods for monocular human reconstruction under occlusion, because occlusion is the main failure mode in our setting.”

% 2. Start with a functional opening paragraph (not a polished one)
% e.g. “A large body of work has studied [problem setting]. Existing approaches can be broadly grouped into [2–3 categories], which differ mainly in [key axis: representation, supervision, assumptions, etc.].”

% 3. Write in “paper clusters,” not paper-by-paper
% - Group papers before writing
% - Write one sentence per group, not per paper

% 4. End each subsection with a bridge sentence
%  “What do all these approaches still fail to address that motivates the next section or my method?”

% ---------

% 5 min – Write the scope sentence (“This subsection reviews…”)

% 10 min – Write 3–5 bullet points describing clusters of work

% 10 min – Turn bullets into rough prose (no citations yet)

% 5 min – Add citations and a closing bridge sentence

\section{Parametric Body Models}
% notes:
% also called statistical body models

% scope sentence: in our work, we rely on parametrric body model to a. anchor our explicit 3d representation and b. to define the deformation field over time.
% clusters:
% - 1. early body models before smpl: 
% issues:
% - traditional approach: model how vertices are related to the underlying skeleton via skinning weights -> while these approaches are supported by rendering engines, they generate unrealistic joint deformations 
% - support limited variety of body shapes
% - some are not compatible with with back then existing graphics pipeines and rendering engines

% goals of smpl: make the model as simple as possible, as well as standardised so that it can be used by the community at large, and keep the realistic defomration and shapes learned from data

% - 2. smpl and its variants (smplh, smplx)
% in general, smpl divides the body modelling into two parts: 
% a. shape model - the question is how to offset each vertex based on the shape parameters
% b. pose model - you have predefined set of joints, and the question is how to rotate each of these joints - they use recursive kinematic tree with blend skinning weights to define how each vertex is influenced by the underlying joints

% smpl stands for skinned multi-person linear model
% smplx - prior works only focus either on body only or hands, not separately - except from an early attempts
% ---Frank model - which however just stitched together seprarate hands, face and body models, this stitching result in unrealistic model
% --- SMPL-H - extends smpl by adding articulated hands however it still does not model the face -> therefore they start from smpl-h and then add face model on top of it (FLAME)
% in addition to smpl, smplx combines smpl body model with FLAME head model and MANO hand model

% both smpl and smpl are models learned from large scale dataset of 3d body scans, smplx says 5586 scans 

% FLAME in contrast to previous approaches that focus only on face models the entire head


% - 3. modern approaches - annybody, meta momentum human rig (mhr)
% mhr decouples skeletal pose from body shape to provide more control and higher interpretability -> they buiold this based on human momentum rig paper which is in turn based on ATLAS 
% annybody - argues that smpl is trained on scans with limited population diversity, and therefore struggles to generalise to for instance children or elderly people. therefore they take scan free approach and learn the body priors instead from the community driven MakeHuman framework


Modelling human bodies is a long standing and important problem in computer vision and graphics. The foundational paper in this area is the Skinned Multi-Person Linear (SMPL) model \cite{smpl}. Unlike its predecesors, SMPL was the first parametric model that was compatible with existing graphics pipelines, differentiable and capable of generaating realistic body shapes and poses since it was fitted on a large corpora of 3D scans. SMPL divides the body modelling into two parts. First, it models the shape variations via learned PCA shape space. Specifically, to obtain shape of given person, SMPL requires 10 shape parameters that define the coefficients of the PCA basis. Together, linear combination of these basis vectors define the offsets for each vertex in the body mesh. Second, the pose of the mesh is modelled via a predefined kinematic tree of joints. During inference, human mesh recovery method estimate the local rotation of each joint in the kinematic tree. The tree is then recursively travsersed to obtain the globabal transformation of each joint. Finally, to define how each vertex in the mesh is influenced by the underlying joints, SMPL uses the learned skinning weights to combine the transformations of all joints affecting given vertex.

While SMPL enabled major step towards realistic human body modelling, it still lacked the ability to model hand and face details. To address this limitation, SMPL-H extended the SMPL model by adding articulated hand modelling \cite{smplh}. Later, SMPL-X extends the SMPL-H by adding also detailed face 
modelling based on the FLAME head model \cite{smplx,flame}, and therefore marks the first method to jointly model body, hands and face in a single parametric model. Most recently, Anny \cite{annybody} introduces a new parametric body model that aims to address the limited population diversity in the training data of SMPL models. Specifically, AnnyBody learns body shape priors from the community driven MakeHuman framework, which contains a wide variety of body shapes including children and elderly people.
Finally, most recently Meta introduced the Momentum Human Rig (MHR) \cite{yang2025sam3dbody}, which decouples skeletal pose from body shape to provide more control and higher interpretability. 

We use SMPl-X body model in our work since it is being predicted by our chosen HMR method. We use the SMPL-X model to define the deformation field over time for our explicit 3D representation and also to 
provide strong geometric prior for human body shape.


\section{Human Motion Estimation}

% why this section: in order to use smpl-x body model, we need to first estimate its parmaeters
% from the monocular video
% 22 methods: glamr
% 23 methods: humans in 4d
% 24 methods: wham, tram, pace
% 25 methods: Hsfm, camera HMR, PromptHMR, GENMO, human3r, sam3d body

% local human mesh recovery: 
% - goal: estimate parameters for mesh based body model from a single image in the local camera frame
% - multiple people:
% -- you can use top down approach: first detect people via object detector, then crop each person and then run single person hmr method, e.g. Humans in 4D, camera hmr
% -- bottom up approach: first estimate all keypoints in the image, then group them into people, then run single person hmr method on each person. This should address the problems with more crowded scenes where people are close to each other. e.g. multi-hmr

% global human mesh recovery:
% - goal: estimate parameters for mesh based body model from a monocular video in the global world frame
% - by defintion, very ill constrained problem since neither the camera nor the human motion is known, and decoupling these is not trivial
% - early methods such as glamr first estimate local human mesh for each frame, then use these as input to the predictor that estimate the translation of the person in the world frame, after this, there is some joint optimisation to refine the results

% - later methods such as slahmr rely on SLAM to estimate camera poses in world frame, and then use these to guide the global human mesh recovery, later TRAM improves Slahmr by enhancing the camera pose estimates to be in meters
% - in contrast to these offline methohds, more modern methods such as TRACE or WHAM take an end to end approach and regress directly the global human mesh parameters from the monocular video and making use of memory unit to store the past information about the scene 


% human3r notes:
% - key idea: to reconstrution human motion accurately, we need to understand their surroundings as well
% x hsfm reconstructions humans and scene separately and then combines them via refinement under constraints
% in general the main advantage of h3r is unified one stop solution that is fast, easy to install and deploy in contrast to more complxe multi stage pipelines

% todo: possible also mention sam3d body here since it is from meta and it is quite recent as well


Human Mesh Recovery (HMR) methods aim to estimate the parameters of parametric body models from images or videos. These methods can be categorised into two main groups: local HMR and global HMR. Local HMR methods focus on estimating body model parameters in the local camera frame from single images, while global HMR methods aim to recover body model parameters in the global world frame from monocular videos.

\paragraph{Local HMR methods}. Majority of the local HMR methods adopts a two-stage top-down approach. First, the method usually uses an off the shelf object detector to detect people in the image. Then, for each detected person, the method crops the image and runs a single-person HMR method to estimate the body model parameters \cite{4dhumans,camerahmr}. However, the top down approach often fails in crowded scenes. Therefore, bottom-up approaches first detect all joint keypoints in the image and then group these into individual per subjects keypoints before running single-person HMR \cite{multihmr}.

\paragraph{Global HMR methods}. Earlier approaches first such as GLAMR \cite{glamr} first run local HMR for each frame, followed by a separate module that estimates the global translation of the person in the world frame. Finally, a joint optimisation is performed to refine the results. Later methods such as SLAHMR \cite{slahmr} and TRAM \cite{tram} leverage SLAM to estimate camera poses in the world frame, which are then used to guide the global HMR. More recent methods such as WHAM \cite{wham} and TRACE \cite{trace} take an end-to-end approach and directly regress global body model parameters from monocular videos, often using memory units to store past information about the scene. While these recent methods achieve already promising results, they still rely on varying number of the shelf estimators such as 2D keypoint detectors, segmentation models, and depth estimators. This dependency becomes espeically problematic in video where tracking consistency is crucial for high quality results.
Most recently, Human3R \cite{chen2025human3r} proposes a unified framework that jointly reconstructs human motion and scene geometry from monocular videos. By jointly reasoning about humans and their surroundings, Human3R achieves more accurate and consistent reconstructions compared to prior multi-stage pipelines. We therefore choose Human3R as our HMR method in this work.





% nerf notes:
    % at the time, there was a new trend to encode scene or 3d object as implicit neural representation (in practice a MLP net) - e.g. sign distance field (paper: A volumetric method for building complex models from range images)
    % however at that time, these neural encoding techniques still lacked behing traditional methods, such as meshes or voxel grids,  in terms of the fidelty of the reconstructiona
    % neural 3d shape representations review:
    % - map xyz coordinates to signed distance values (e.g. deep sdf), or occupancy values (e.g. occupancy networks) -> both these examples required 3D supervision
    % - later in the paper "Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision" they were first to show how to construct differentable rendering pipeline for implicit representations (before them some works showed this for explicit representations such as voxels or meshes), another work Scene Representation Networks learns a net that for each 3d world coordinate outoputs feature vector and rgb color
    % - the prioblem with these techniques is that they were limited to only simple shapes - in contrast nerf showed how to reconstruct complex real world scenes with high fidelity
    
    % View synthesis and image-based rendering:
    % - early techniques relied on light field sample interpolation (e.g. light field rendering, lumigraph) -> these require very very dense capture of images
    % - therefore later methods focused on differentable rendering - one of the main directions focused on optimisation of mesh representation with apperance modeled via diffuse or view dependend apperance -> these did not succeed because apparantly optimising mesh from images is quite hard problem - difficult loss landscape 
    % - therefore later works switched from meshes to volumetric representations - e.g. Deepview, local light field fusion - these approaches train deeped neural net on large number of scene datasets to predict sampled volumetric representation for novel scenes -> the main limit of these volumetric scenes is the resolution - to render higher resoliton iamges, you also need to have denser volumetric grids which quickly becomes infeasible in terms of memory consumption as well as sampling time
    % - nerf overcomes this by using implicit continuous representation that can be sampled at arbitrary resolution, requires less storage, and yileds significantly better quality results


% cat 3d notes:
    % advances in photogammetry unlocked creation of high quality 3d assets from 2d inputs (images / videos) - more precisely NeRF and 3DGS have been the two most influential representations in this area
    % both methods yield highly realistic results, however under the assumption that the input to these method is a dense capture of images  which is rare in real world scenarios
    % so this is what spared interest in 
    % key idea in cat3d paper:
    % "This way, we reformulate a difficult ill-posed reconstruction problem as a generation problem: given any number of input images, generate a collection of consistent novel observations of the 3D scene"
    % -> their solution to the problem: multi-view diffusion model that given arbitrary number of input images can generate novel view images of the scene from arbitrary camera viewpoints

    % the key question in the 3d literature at the very start was how to make use of the fact that 2D data is abundant, whereas 3D data is scarce
    % DreamFusion came up with a fundmental idea of score distlation sampling (SDS) that allows to distil knowledge from 2D diffusion models into 3D representations such as NeRF

    % while this was a major step forward, DreamFusion lacked explicit camera control. This limitation was addressed by subsequent works such Zero123 or zero-nvs that introduced explicit camera conditioning into the diffusion models for 3D generation.

    % another important aspect of 3d reconstruction methods is ensuring multi-view consistency, methods such as Image Dream or cat3d encode this by using 3D attention mechanism in their diffusion models, i believe also recon fusion is a good example here

    % finally, in contrast to synthesise views and then fit NeRF or 3DGS to these views, there has been a new line of works focusing on feedforward 3d reconstruction that maps directly the input modality to the 3d representation.
    % example of such methods in the 3d space include: lrm, dmv3d, instant3d, splatter image, gs-lrm
    % while these methods are extremelly fast, they reconstruction quality is still lagging behind the optimisation based methods


% cat 4d notes:
    % similar to 3d recosntruction, under the right cappture conditions, we can reconstruct high quality 4d representations of dynamic scenes - e.g. 4dgs, free time gs, d-nerf, dynamic 3dgs
    % while for both 3d and 4d, these capture conditions are quite limiting in real world scenarios, it is still feasible to do multi-view capture of a static scene e.g. just walk around with the smartphone. In contrast, 
    % for 4d scene reconstruction, this is a different story - you literally need multiple cameras to capture the dynamic scene from different viewpoints at the same time - so the bar is much higher here

    % the key idea behind cat4d paper is: learn 4d generative model based on the mix of synthetic and real world multi-view dynamic scene data, therefore, their model learns to transform an input monocular video
    % into multi-view video, "Specifically, our model accepts any number of input images captured at different viewpoints and times, and synthesizes as output the scene’s appearance at any specified novel viewpoints and novel times"

    % dynamic 3d scene reconstruction
    % - methods assumes multi-view video capture as input: e.g. hexplane, k-planes, dynamic 3dgs, nerfies, hyper nerf, 4dgs
    % - methods focus on monocular video input: e.g. Neural Scene Flow Fields,  Mosca, shape of motion or most recently HiMor -> they key idea behind these method is to use 3dgs or nerf as the underlying canonical static representation, and learn deformation field over time to model the motion -> the issue with these methods is that they cant reconstruct what has not been observed in the input video. This becomes especially problematic when using explicit representation such as 3dgs which becomes underoptimal in sparse view settings. of course another disadvantage of these methods is that they have to rely on other off the shelf modules 


    % 4d generation:
    % - early methods relied on score distilation sampling - e.g. DreamFusion for 3d, and text-to-4d dynamic scene generation for 4d
    % - further advances in this field were driven by relying on pretrained multi-view (e.g. zero-1-to-3, mvdream) or video diffusion models (most promiment base video diffsion is stable video diffsion model), e.g. dreamgaussians4d, 4d-fy, consistent4d 

    % most recent method drop the sds optimisation, and instead focus on generating multi-video, e.g. dimension-x, cat4d, vidid-zoo, efficient-4d, diffusion^2
    % some methods take it even further and directly train feedforward 4d reconstruction models, e.g. l4gm
    % why can not you use simple video to video diffusion models for 4d gen: no explicit multi-view consistency


    % some notes on the initial score distaltion sampling idea:
    % - the key idea of sds is as follows: render your 3d representation, and then ask diffusion model to refine it, and then backprop use the added vs predicted noise to guide the direction of the update
    % - the problem with this is that the direction of the refiment depends on the input condition - in the vanila version of the sds formulation - this would be text - this is very weakly constraining signal for 3d reconstruction
    % - another issue is that you have to keep prompting diffusion model during training which is quite computationally expensive

% geometry informed video diffusion models
% - e.g gen3c: predicted point clouds from the input image depth maps, and use these as input condition to the diffusion model
% - viewcrafter - init point cloud, render, syntheseize novel views, and repeat


\section{3D and 4D Reconstruction from Dense Input Capture}

In this section, we review prior works that focus on 3D and 4D reconstruction of a given scene under the assumption of having as input dense set of views or multi-view video capture of the scene. While these capture conditions are quite limiting (espeically for the 4D case),
they lay the foundation for the later works that focus on more practical real world capture settings with sparse input views or monocular videos, which we discuss in the next section.


% 3d methods:
% - task definition: 3d reconstruction: estimate geometry and apperance of the scene from a set of images
% - classical structure from motion pipeline such as colmap, map set of images to sparse point cloud via feature matching, triangulation, bundle adjustment, densify the sparse point cloud via multi view stereo
% - neural representation methods:
% -- early neural representation before nerf: main limitation of these methods is that they require 3d supervision (deep sdf, occupancy networks), or can only capture simple shapes ("Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision, Scene Representation Networks") or rendering at higher resolution is very expensive in terms both memory and time (e.g. Deepview, local light field fusion)
% -- nerf overcomes this by using implicit continuous representation that can be sampled at arbitrary resolution, requires less storage, and yileds significantly better quality results with this being said, due to its implicit nature, nerf was slow to train as well as render from
% -- this is why 3dgs became so popular since it addresses these two key limitations of nerf - fast to train and fast to render, while still yielding high quality results, later methods such as 2dgs or mesh splatting focused on also improving the mesh extraction quality from 3dgs

\paragraph{3D reconstruction methods.} The goal of 3D reconstruction techniques is to recover the chosen scene representaton from the given set of images. Classical structure from motion pipelines such as COLMAP \cite{colmap} first map the set of images to a sparse point cloud via feature matching and triangulation, followed by bundle adjustment to refine the camera poses and 3D points. Finally, the sparse point cloud is densified via multi-view stereo techniques \cite{sfmrevisited,schoenberger2016mvs}. While COLMAP recovers accurate camera poses and point clouds, it is not capable of reconstructing continuous surface. This motivates the later neural representations such capable of reconstructing continuous 3D geometru such as DeepSDF \cite{park2019deepsdflearningcontinuoussigned} and Occupancy Networks \cite{mescheder2019occupancynetworkslearning3d}. Main limitation of DeepSDF and Occupancy Networks is that they require 3D supervision during training. Later works such as Differentiable Volumetric Rendering \cite{niemeyer2020differentiablevolumetricrenderinglearning} and Scene Representation Networks \cite{sitzmann2020scenerepresentationnetworkscontinuous} show how to learn implicit neural representations without 3D supervision by introducing differentiable rendering pipelines. However, these methods are limited to reconstructing simple shapes only. NeRF \cite{mildenhall2020nerfrepresentingscenesneural} overcomes these limitations by using implicit continuous representation that can be sampled at arbitrary resolution, requires less storage, and yields significantly better quality results. However, due to its implicit nature, NeRF is slow to train as well as render from. This motivates the later explicit representations such as 3D Gaussians (3DGS) \cite{3dgs} that address these two key limitations of NeRF - fast to train and fast to render, while still yielding high quality results. Later methods such as 2D Gaussians \cite{2dgs} and Mesh Splatting \cite{held2025meshsplattingdifferentiablerenderingopaque} focus on improving the mesh extraction quality from 3DGS. In our work, we choose 3DGS as our underlying 3D representation due to its fast training and rendering speed.



% 4d methods:
% - 4d task definitiion: same as 3d task definition but also need to model motion of the underlying scene representation
% - prior neural representation methods: volume deform or dynamic fusion - rely on rgb-d data, and iteravily fit the underlying 4d model
% - emphasis put on modelling motion: shape of motion, mosca, 4dgs, free time gs

\paragraph{4D reconstruction methods.} The goal of 4D reconstruction techniques is to recover the spatio-temporal scene representation from multi-view video capture of the dynamic scene. Early works such as VolumeDeform \cite{volumedeform} and DynamicFusion \cite{dynamicfusion} rely on RGB-D data and iteratively fit the underlying 4D model. Later works focus on modelling motion of the underlying scene representation. For instance, Shape of Motion \cite{som} and MOSCA \cite{lei2024moscadynamicgaussianfusion} use 3DGS as the underlying 3D representation and learn deformation field over time to model the motion. In contrast, 4D Gaussians (4DGS) \cite{4dgs} extend 3DGS to dynamic scenes by adding time as an additional input to the representation. Free Time Gaussians \cite{freetimegs} builds on top of 4dgs and introduce topology changing representations that allow Gaussians to appear and disappear over time, enabling modelling of dynamic scenes that capture complex motion. In contrast to methods such as Shape-of-Motion or Mosca that rely on point tracking to define the deformation field, we use parametric body models to define the deformation field over time. We hypothesize that parametric body prior provides stronger regularization for human motion compared to point tracking, leading to more accurate and consistent motion estimation.


\section{3D and 4D reconstruction from Sparse Input Capture}
% More practical real world settings capture
%  in practice, you have access either to image, set of sparse image or monocular video. In this case, you need to densify the scene
% -----------------------

% 3d methods 
% - earlier methods focused on better constraining the optimisation process of 3dgs / nerf via geometric prioors: mipnerf 360 (ray entropy), reprojection lossses (sparf), regnerf
% - other rely on off the shelf modules to estimate depth / normals, e.g. "Dense depth priors for neural radiance fields from sparse input views", sparsenerf, fsgs, monosdf, depth supervise nerf
% --- it seems that one of the main limitations of these methods is that they only yiled marginal improvements in denser view settings, also hard to balance the losses
% - most recent focus is on synthesising new views via generative models to densify the scene:
% -- distilation sampling methods: score (dreamfusion, prolific dreamer), neural mode seeking (sparse fusion), score jacobian chaining (zero1-to-3), nerf guided distilation (nerf diff)
% ---- main disadvantages: expensive to query diffusuin modesl during trainign, ill posed conditioning signal for the diffusionm model guidanc, focus on object centric scenes and scale poorly to larger scenes 
% -- synth views and then backrpoject: e.g. reconfusion, view crafter
% -- synth new views first and then fit 3d rep: e.g. cat3d, sv3d
% -- refine images and add as extra training views: flowr / difix / 3dgs enhancer: new way of using diffusion models - instead of doing noise to data generation, they do conditional source distribution to target distribution generation: 
%   "We hypothesize that a generative process that directly maps samples from a conditional source distribution of rendered images to the target distribution of ground truth images is **preferable** to conditional noise-to-data generation" flowr compare to difix is also using flow matching, they propose single multi-view diffusion model that given M observed images and N rendered images, synthesises N refined images in a single forward pass ensuring multi-view consistency 
% - feedforward methods 
% --- MVS nerf and pixel nerf - first significant approach to tackle the generalisability for Nerf, LRM proposes eed forward model to predict directly 3d objrc represeantion from single iamge, trained on large dataset. the emphasis on object. 
% --- more recently methods focused on directly predicint 3d representation from sparse set of images - pixelsplat, mvsplat, mvsplat360, lgm
% --- For the scenes level recon that predicts also camera params, first shift came with dust3r, mast3r which take pair of images and predict camera parameters along with sparse point cloud (not sure what is the rep exactly). The main downside of these methods was the reliance on pair of images only, then vggt came and show it can infer all the 3d geometry from single image. Follow methods such as cut3r or point3r added a memory and use it to store past information about the scene to improve the quality of the reconstruction. bolt3d isntead predicts 3dgs instead of point clouds 

% 4d methods
% - two stage - synthesise and then optimise: synthesise first multi view video, then fit 4d representation -> separation of concerns - densification and 4d reconstruction
% --- e.g. cat4d, sv4d
% - altenation between synthesise and optimise 4d rep: construct first a sparse representation of the scene, render it, improve the renderings, repeat -> joint densification and 4d reconstruction
% --- e.g. bulletgen
% - feed forward with the focus on getting the camera and some sort of point bases representation: monst3r (focusing on extending dust3r to dynamic setting - their issue was that due to their formulatio you still required a 2D tracker 4d motion), dynamic points maps build on the idea of dust3r / mast3r and their view invariant point maops and introduced view and time invariant point maps to model dynamic scenes from monocular video -> this representaaion is complete in a sense that it can reconstruct all the components of the 4d scene including motion (without requiring extra 2d tracker module, similar formulation was adapted by St4track. Other feed forward approaches such as align3r, cut3r pi3 recover and align depth maps but still require 2d tracker or video diffusion (case of geo4d) to model motion. Another trully feed forward method is any4d that models motion via scene flow and geometry via dense point maps. feedforward with focus on dynamic objects: l4gm, animate3d.


% human centric methods 
% -------------------------

% - implicit reps: human nerf, neuman, instant nvr, vid2avatar (no gen component)
% - explicit reps: 
% -- multi view video: free time gs, tao gs
% -- mono video: 
% --- no generative prior: drivable 3dgs, 3dgs avatar, gauhuman, gaussian avatar, hugs, exavatar, vid2avatarpro + hsr and odhsr (that add scene to the reconstruction) + MultiPly that focuses on multiple humans
% --- with generative prior: mv performer (single human), guess the unseen (multiple humans)
% -- single or sparse set of images: ani-gs, fresa, adahuman, human ram, mv performer