%% ----------------------------------------------------------------------------
% CVG SA/MA thesis template
%
% Created 03/08/2024 by Tobias Fischer
%% ----------------------------------------------------------------------------

% -------- Instructions for writing the method section:
% The objectives of the Method section are the following: 
% \begin{itemize}
 
 % \item \textit{What problem are you solving?} Give a clear, formalized statement of the problem you are solving, \ie which inputs are given, which assumptions you make, and what is the desired output (goal).
 % \item \textit{What are the preliminaries of your work?} Introduce the environment in which your work has taken place - this can be a software package, a device, or a system description. Ensure sufficiently detailed descriptions of the algorithms and concepts (e.g. math) you used.
 % \item \textit{What is your work?} Describe the key component of your work, e.g. an algorithm or software framework you have developed.
% \end{itemize}


\newpage
\chapter{Method}

\section{Preliminaries}

\subsection{SMPL-X Body Model}
Skinned Multi-Person Linear Model eXtended (SMPL-X) \cite{smplx} is a parametric 3D human body model that represents a minimally clothed human body (it does not model clothing geometry). It defines parameters that control both body shape and articulation. Shape parameters $\beta$ capture person-specific attributes such as body proportions and are constant across frames. Pose parameters $\theta$ control the articulation of the body (and, in SMPL-X, also hands and face). Given these parameters, SMPL-X maps a canonical template mesh to a posed mesh using forward kinematics and linear blend skinning (LBS). In our work, we use SMPL-X as a strong geometric prior to position and deform our explicit 3D Gaussian Splatting representation. We estimate the SMPL-X parameters using pretrained feed forward model Human3R \cite{chen2025human3r}. In the remainder of this section, we go over the posing process as defined in the SMPL-X model \cite{smplx}. For more details we refer the reader to the original SMPL-X paper \cite{smplx}.

\paragraph{SMPL-X model overview.} In general, SMPL-X is a mesh-based model defined as the following function:

\begin{equation}
M(\beta, \theta, \psi, t) \rightarrow V \in \mathbb{R}^{N \times 3}
\end{equation}

\noindent where $\beta$ are the shape parameters, $\theta$ are the pose parameters, $\psi$ are the facial expression parameters, $t$ is the global translation of the body in the world coordinate system, and $V$ are the output mesh vertices of the body with $N$ vertices \cite{smplx}.

\paragraph{Fixed model components provided by SMPL-X.}
SMPL-X comes with fixed model components, including the template mesh $T_{0}$, a kinematic tree (joint hierarchy), a joint regression function $J(\beta)$, per-vertex skinning weights $w_{j,i}$ that specify how strongly joint $i$ influences vertex $j$, and learned blendshape bases used by $B_{s}(\beta)$, $B_{e}(\psi)$, and $B_{p}(\theta)$. The skinning weights and blendshape bases are part of the SMPL-X model and are not estimated in our pipeline. Preprocessing estimates per-person shape $\beta$ and, for each frame, the pose parameters $\theta$ and global translation $t$. We do not use facial expression parameters $\psi$. For reference, the shape blendshapes are implemented as a linear combination of fixed per-vertex direction fields,
\begin{equation}
B_{s}(\beta) = \sum_{k=1}^{|\beta|} \beta_{k}\mathbf{S}_{k},
\end{equation}
where $\mathbf{S}_{k}\in\mathbb{R}^{N\times 3}$ are shape directions learned during SMPL-X model training and $\beta_{k}$ are the corresponding shape coefficients.

\paragraph{Applying shape parameters.} We start from the template mesh in the rest pose (all joint rotations set to zero), denoted
$T_{0} \in \mathbb{R}^{N \times 3}$. We then apply the shape parameters $\beta$ using blendshapes as follows:

\begin{equation}
T_{shaped} = T_{0} + B_{s}(\beta)
\end{equation}
where $B_{s}(\beta)$ is the linear blendshape function that takes as input the shape parameters $\beta$ and outputs the per-vertex offsets to be applied to the template mesh. Optionally, we can also apply expression blendshapes $B_{e}(\psi)$ to account for facial expressions:

\begin{equation}
T_{shaped,expr} = T_{0} + B_{s}(\beta) + B_{e}(\psi)
\end{equation}
In practice, in our work, we do not use facial expressions because Human3R does not estimate them reliably, so we only apply the shape blendshapes.

\paragraph{Converting pose parameters into global transformations.} Next, we convert the pose parameters $\theta$ from axis-angle representation to rotation matrices using Rodrigues' formula. As a result, for each of the $K$ joints in the kinematic tree of the SMPL-X body model we obtain a rotation matrix $R_{i} \in \mathbb{R}^{3 \times 3}$ where $i = 1, \ldots, K$ (in our setup, $K=55$). These rotation matrices represent the local joint rotations with respect to the parent joint in the kinematic tree.

\paragraph{Pose-dependent deformations.}
In addition to shape and expression blendshapes, the standard SMPL-X formulation includes pose-dependent blendshapes $B_{p}(\theta)$, which model non-linear pose effects such as muscle bulging \cite{smpl}. These offsets are applied in the canonical space before skinning. We rely on the standard SMPL-X implementation \cite{smplx} for these details.

We convert each of these local joint \textit{rotations} to local joint \textit{transformation} matrices $G_{i} \in \mathbb{R}^{4 \times 4}$ by appending the joint translation and a row for homogeneous coordinates as follows:

\begin{equation}
G_{i} = \begin{bmatrix} R_{i} & t_{i} \\ 0 & 1 \end{bmatrix}
\end{equation}
where $t_{i} \in \mathbb{R}^{3}$ is the translation vector of joint $i$ with respect to its parent joint defined precisely as follows:

\begin{equation}
t_{i} = J_{i}(\beta) - J_{parent(i)}(\beta)
\end{equation}

where $J_{i}(\beta)$ is the 3D position of joint $i$ given the shape parameters $\beta$, and $parent(i)$ returns the index of the parent joint of joint $i$ in the kinematic tree. 

Finally, to transform the rigid transformations from the local joint space to the world coordinate system, we perform forward kinematics as follows:

\begin{equation}
G_{i}^{world} = G_{parent(i)}^{world} \cdot G_{i}
\end{equation}
where $G_{i}^{world}$ is the world transformation of joint $i$, and $G_{parent(i)}^{world}$ is the world transformation of the parent joint of joint $i$. We perform this operation recursively starting from the root joint.

\paragraph{Posing the mesh via linear blend skinning and global pose transformations.} Finally, we pose the mesh vertices using linear blend skinning (LBS) and the global joint transformations obtained from forward kinematics. Each vertex $v_{j}$ in the mesh is associated with a set of fixed skinning weights $w_{j,i}$ for each joint $i$ (see the model overview above). The posed vertex position $v_{j}^{posed}$ is then computed as follows:

\begin{equation}
v_{j}^{posed} = \sum_{i=1}^{K} w_{j,i} \cdot \left(G_{i}^{world} \cdot \begin{bmatrix} v_{j}^{shaped} + B_{p}(\theta)_{j} \\ 1 \end{bmatrix}\right)_{1:3}
\end{equation}
where $v_{j}^{shaped}$ is the vertex position after applying shape blendshapes, $B_{p}(\theta)_{j}$ is the pose-dependent offset for vertex $j$, and $(\cdot)_{1:3}$ extracts the first three components of the resulting homogeneous coordinate. This operation is performed for all vertices in the mesh to obtain the final posed mesh $V^{posed}$.

\paragraph{Adding global translation.} To complete the posing process, we add the predicted global translation $t$ to all posed vertices as follows:
\begin{equation}
V^{final} = V^{posed} + t
\end{equation}
where $V^{final}$ is the final posed mesh with global translation applied.


\subsection{3D Gaussians}

We use 3D Gaussians \cite{3dgs} as an explicit representation for each of the humans in the scene. In this section, we summarize the key concepts of 3D Gaussian Splatting (3DGS) relevant to our work. For a more detailed explanation, we refer the reader to the original 3DGS paper \cite{3dgs}.

\paragraph{Representation.} Each Gaussian is parameterized by its center position $\boldsymbol{\mu}\in\mathbb{R}^{3}$, an anisotropic scale $\boldsymbol{\sigma}\in\mathbb{R}^{3}$ (diagonal standard deviations), a rotation quaternion $\mathbf{q}\in\mathbb{R}^{4}$, an opacity $\alpha\in\mathbb{R}$, and a color $\mathbf{c}\in\mathbb{R}^{3}$ (RGB in our implementation). Together, these parameters define an oriented anisotropic density blob in 3D. 

\paragraph{Projection of 3D Gaussians to 2D.}
Given a set of 3D Gaussians $\{\mathcal{G}_{i}\}_{i=1}^{M}$ and a calibrated camera, we render images using differentiable Gaussian splatting \cite{3dgs}. We denote the camera intrinsics by $\mathbf{K}\in\mathbb{R}^{3\times 3}$ and the camera extrinsics by a world-to-camera transform $\mathbf{T}_{\mathrm{w2c}}=[\mathbf{R}\,|\,\mathbf{t}]\in\mathrm{SE}(3)$ with $\mathbf{R}\in\mathrm{SO}(3)$ and $\mathbf{t}\in\mathbb{R}^{3}$.
Each Gaussian $\mathcal{G}_{i}$ is defined by a 3D mean $\boldsymbol{\mu}_{i}$ and a 3D covariance $\boldsymbol{\Sigma}_{i}$. Note that we follow the same parameterization as introduced in the original 3DGS work \cite{3dgs}, i.e., the covariance is constructed from the anisotropic scales and rotation as $\boldsymbol{\Sigma}_{i}=\mathbf{R}_{i}\mathrm{diag}(\boldsymbol{\sigma}_{i}^{2})\mathbf{R}_{i}^{\top}$, where $\mathbf{R}_{i}$ is the rotation matrix corresponding to the quaternion $\mathbf{q}_{i}$. Given these inputs, we first transform the mean and covariance to camera coordinates:
\begin{equation}
  \boldsymbol{\mu}_{i}^{c} = \mathbf{R}\boldsymbol{\mu}_{i} + \mathbf{t}, \qquad
  \boldsymbol{\Sigma}_{i}^{c} = \mathbf{R}\boldsymbol{\Sigma}_{i}\mathbf{R}^{\top}.
\end{equation}
We then project the Gaussian to the image plane using a first-order approximation of the pinhole projection as done in \cite{3dgs} as well. Let $\pi(\cdot)$ denote the standard pinhole projection with intrinsics $\mathbf{K}$, which maps a camera-space point to pixel coordinates by perspective division followed by multiplication by $\mathbf{K}$. We denote by $\mathbf{J}_{i}=\frac{\partial \pi}{\partial \mathbf{x}}\big|_{\boldsymbol{\mu}_{i}^{c}}$ the Jacobian of this projection evaluated at the Gaussian mean \cite{3dgs}. The projected mean and covariance are
\begin{equation}
  \mathbf{u}_{i}=\pi(\boldsymbol{\mu}_{i}^{c}), \qquad
  \boldsymbol{\Sigma}_{i}^{\mathrm{2D}} = \mathbf{J}_{i}\boldsymbol{\Sigma}_{i}^{c}\mathbf{J}_{i}^{\top}.
\end{equation}
Intuitively, after projection each 3D Gaussian becomes a soft 2D ellipse in the image. The ellipse is centered at the projected mean $\mathbf{u}_{i}$ and its shape and size are determined by the projected covariance $\boldsymbol{\Sigma}_{i}^{\mathrm{2D}}$. 

\paragraph{Rasterization.}
Given the projected ellipses, rasterization turns their per-pixel contributions into the final rendered image by accumulating colors and opacities along each camera ray \cite{3dgs}. Specifically, we start by computing \textit{for each pixel} the weight of its surrounding Gaussians which fall within a predefined radius. The weight for each such Gaussian with index $i$ at pixel location $\mathbf{u}$ is computed as:

\begin{equation}
  w_{i}(\mathbf{u}) = \,\exp\!\left(
  -\tfrac{1}{2}(\mathbf{u}-\mathbf{u}_{i})^{\top}(\boldsymbol{\Sigma}_{i}^{\mathrm{2D}})^{-1}(\mathbf{u}-\mathbf{u}_{i})
  \right)
\end{equation}
In words, we assign higher weights to Gaussians that are closer to the pixel center $\mathbf{u}$, with the falloff determined by the projected covariance $\boldsymbol{\Sigma}_{i}^{\mathrm{2D}}$. Before computing the final pixel color, we sort the Gaussians along the camera ray based on their depth (distance from the camera). We then compute the final pixel color using front-to-back alpha compositing as follows:

\begin{equation}
  a_{i}(\mathbf{u}) = \alpha_{i}\,w_{i}(\mathbf{u})
\end{equation}

\begin{equation}
  T_{i}(\mathbf{u}) = \prod_{j=1}^{i-1}\left(1-a_{j}(\mathbf{u})\right)
\end{equation}

\begin{equation}
  \mathbf{C}(\mathbf{u}) = \sum_{i=1}^{M} T_{i}(\mathbf{u})\,a_{i}(\mathbf{u})\,\mathbf{c}_{i}
\end{equation}
Here, $a_{i}(\mathbf{u})$ is the effective opacity contributed by Gaussian $i$ at pixel $\mathbf{u}$, computed from its learned opacity $\alpha_{i}$ and its spatial footprint weight $w_{i}(\mathbf{u})$. The accumulated transmittance $T_{i}(\mathbf{u})$ is the fraction of light that reaches Gaussian $i$ after passing through all closer Gaussians \cite{3dgs}. The final pixel color $\mathbf{C}(\mathbf{u})$ is then obtained by summing each Gaussian's color $\mathbf{c}_{i}$ weighted by its effective opacity and the accumulated transmittance.

\paragraph{Making use of rendering library.} In practice, we use the gsplat library \cite{ye2025gsplat} to handle the whole rendering process, including projection and rasterization. Importantly, the whole process is differentiable, which allows us to optimize the 3D Gaussian parameters.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/preprocessing_overview.drawio.png}
    \caption{\textbf{Preprocessing overview}. Overview of our preprocessing pipeline with all its outputs and off-the-shelf models used to produce them.}
    \label{fig:preprocess_overview}  
\end{figure}

\section{Problem Definition}

Our input consists only of the RGB frames of a monocular video capturing multiple people interacting in a dynamic scene. The video can be captured with either a static or a moving camera. Since no camera parameters, segmentations, depth maps, or body model parameters are given as input, we first run a preprocessing stage that uses off-the-shelf methods to obtain the estimates required to initialize training. These include (i) per-person SMPL-X pose parameters, (ii) per-frame camera parameters, (iii) per-person instance masks, (iv) per-frame depth maps, and (v) a per-person canonical 3DGS initialization. Figure~\ref{fig:preprocess_overview} shows each of these outputs along with the off-the-shelf models used to obtain them. We describe the preprocessing stage in more detail in the next section.

These estimates already provide a solid baseline reconstruction. We then apply our proposed training pipeline to further optimize this baseline to (i) improve pose estimates and (ii) improve appearance (novel view synthesis). Unless stated otherwise, camera parameters, masks, and depth remain fixed during training, while we alternate between optimizing SMPL-X parameters and 3DGS parameters (Section~\ref{sec:training-objectives}). The final output is a 4D scene representation that includes a canonical 3DGS model, per-frame SMPL-X parameters for each person, and per-frame camera parameters. If needed, the 3DGS can be converted to a mesh representation for visualization and evaluation. We represent geometry in meters.

Our evaluation axes reflect the overall objectives of the thesis. First, we target high-quality appearance from arbitrary viewpoints, which we assess via novel view synthesis. Second, we aim for realistic geometry and motion, which we assess via pose estimation and mesh reconstruction metrics. Third, we aim for fast training, which we assess via training speed measurements. We define the specific metrics and evaluation protocols in the Experiments section.

\section{Preprocessing}
Our pipeline relies on a preprocessing stage that estimates camera and human priors from the raw monocular video (Figure~\ref{fig:preprocess_overview}). We first estimate per-frame camera parameters and per-person SMPL-X parameters using Human3R \cite{chen2025human3r}, which outputs all estimates in a shared world coordinate frame and in metric units. Next, we run SAM3 \cite{carion2025sam3segmentconcepts} to obtain per-person instance segmentation masks and rely on its temporal tracking to maintain consistent identities across frames. Finally, we initialize a canonical 3DGS model for each person using LHM \cite{qiu2025lhm} by masking the first frame with the corresponding SAM3 instance mask and feeding the masked image to LHM, which predicts Gaussians in a neutral canonical pose space. This initialization is later crucial for novel training view synthesis, where we render posed 3DGS and refine them with DiFix - if the initial renderings are poor, the image-to-image refinement becomes substantially harder.

% \paragraph{Limitations.}
% Preprocessing can fail in several ways. SAM3 may miss all people in a scene or lose a person for a subset of frames, which requires manual inspection. In addition, mask track identities may not always match the motion tracks and ground truth track identities, and we need to ensure that we process all detected people. These limitations motivate careful curation of the preprocessed outputs before training.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/qual_poor_nvs_from_mono_trn.drawio.png}
    \caption{\textbf{Example of poor novel view synthesis from a monocular input}. When training a 3DGS representation from a single monocular viewpoint without additional synthesized views, the resulting renders from novel angles often contain blur, holes, and missing content.}
    \label{fig:qual_poor_nvs_from_mono}  
\end{figure}


\section{Training View Synthesis with DiFix}
% Optional improvements (to revisit later):
% 1. Better explanation of the DiFix model
% 2. Discussion to why we do dense temporal supervision but sparse viewpoint supervision
% 3. I could show what happens if you do not provide additional synthesized views 

% inclue this figure here:
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/example_of_progressive_trn_nv_gen.drawio.png}
    \caption{\textbf{Example of progressive novel training view synthesis}. We start by synthesizing the closest cameras to the source camera view (first column). Once we complete this step, we repeat the same procedure for the next set of cameras farther away from the source camera view (second column). This time, we use the previously synthesized views as reference views.}
    \label{fig:how_we_choose_ref_and_tgt_nv_views}  
\end{figure}

Our explicit 3DGS representation is sensitive to the number of input views. When trained from a single monocular viewpoint, the Gaussians are optimized mainly to explain the source camera observations and can remain poorly conditioned for unseen viewpoints, for example with incorrect opacity, scale, or appearance. As a result, renders from novel angles often contain blur, holes, and missing content, as shown in Figure~\ref{fig:qual_poor_nvs_from_mono}. This is a key difference compared to implicit representations such as signed distance fields, which typically generalize more smoothly under sparse-view supervision. In particular, our main baseline MultiPly \cite{multiply} uses an implicit SDF-based representation and therefore does not require additional synthesized cameras to the same extent as our explicit Gaussian representation.

To obtain a representation that can be rendered from arbitrary viewpoints, we densify supervision by synthesizing additional training views around the scene. In our experiments, we use one source camera and synthesize seven novel training cameras per scene.


\paragraph{Progressive view synthesis.} Figure~\ref{fig:how_we_choose_ref_and_tgt_nv_views} demonstrates how we progressively generate novel training views. We start from the source camera view and synthesize nearby camera positions first. Once these are generated, we use the previously synthesized neighboring view in the traversal order as the reference view to synthesize the next, farther camera. This progressive approach relies on the fact that nearby views have large overlap, so the image-to-image model only needs to apply small changes at each step.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/single_nv_train_view_synthesis_diagram.drawio.png}
    \caption{\textbf{Example of synthesizing a single novel training view}. We sample frame $t$ from the reference camera view. We then use our canonical 3DGS human scene representation initialized from LHM's prediction, estimated SMPL-X parameters, and estimated camera parameters to render an initial novel view. Finally, we feed the reference view, the initial novel view, and a text prompt to DiFix to obtain the refined novel view at time $t$. We repeat this process for all frames in the video to obtain a full refined novel view video for the target camera.}\label{fig:how_we_synthesise_single_nv_view}  
\end{figure}


\paragraph{Novel view refinement using DiFix.} Figure~\ref{fig:how_we_synthesise_single_nv_view} illustrates the synthesis of a single novel training view at a target camera position. We start by rendering an initial novel view using our canonical 3DGS representation (initialized from LHM) together with the estimated SMPL-X and camera parameters. We then refine this initial rendering using DiFix \cite{wu2025difix3d}, an image-to-image diffusion model designed for high-quality view synthesis. DiFix takes as input the reference view, the initial novel view, and a text prompt that describes the desired transformation (in our case, ``remove degradation''). The model outputs a refined novel view that follows the geometry and perspective of the initial rendering while restoring high-frequency details from the reference view. We repeat this process for all frames to obtain a refined video for the target camera. Figure~\ref{fig:example_of_all_cams_in_3d} visualizes the full set of training cameras in 3D, where the blue camera denotes the source view and the remaining cameras are the virtual viewpoints used for novel view synthesis.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/example_of_all_cams_in_3d.png}
    \caption{\textbf{Example training cameras in 3D}. Blue camera denotes the source view while the remainder are virtual cameras used for novel view synthesis.}\label{fig:example_of_all_cams_in_3d}  
\end{figure}

\paragraph{Key design principles behind our synthesis approach.} Importantly, our synthesis is anchored in geometry. For every target camera, we render an initial view by projecting the same posed 3DGS representation into the target viewpoint, and DiFix then operates as a refinement model on top of this geometrically grounded rendering. As a result, DiFix does not need to infer human pose or global scene structure from scratch and can focus on restoring missing details and correcting local artifacts, which reduces hallucinations and improves multi-view consistency. This setup also encourages temporal consistency. Motion is driven by the estimated per-frame SMPL-X parameters that are shared across all synthesized views, and DiFix primarily refines appearance rather than inventing new motion. In contrast, a video diffusion model asked to generate multi-view videos directly from a monocular input and target cameras has more freedom and can hallucinate view-dependent motion inconsistencies across generated videos.


\section{3DGS-based Dynamic Representation}

% Terminology definitions:
% 1. Neutral pose - our own pose definition which is similar to A pose (have to double check this)
% 2. Zero pose - SMPLX zero pose definition (all pose parameters set to zero)
% 3. Target frame pose - estimated SMPLX pose for a given frame in the video sequence
% 4. N = number of 3dgs vertices after upsampling = 40k

% We define the neutrally posed upsampled mesh vertex as follows:
% 1. We first define a neutral pose for the SMPLX model - this is similar to A-pose
% 2. We then obtain the neutral pose mesh vertices - approximately 10k vertices
% 3. We upsample these vertices to 40k vertices using the body/face ratio: 3 -> we therefore end up with around 30k body vertices and 10k face vertices
% 4. For each upsampled vertex, we also obtain the corresponding LBS weights for all 55 joints by finding the closest original vertex from the original 10k vertices -> we get 40k x 55 LBS weights matrix
% In addition, we store the transformation that takes us from the neutral pose back to the zero -> matrix of shape (55, 4, 4)


% LHM then predicts the canonical 3DGS parameters in the neutral pose space:
% 1. mean offsets from the neutral pose vertices, shape Nx3
% 2. rotation represented as quarternions, shape Nx4
% 3. scales represented as diagonal covariance matrices, with shape Nx3, therefore each of the 3 dims defines the variance along the corresponding axis
% 4. opacity, shape Nx1
% 5. colors in RGB space, shape Nx3

% Deformation from the neutral pose to the target frame pose then works as follows.
% On a high level, we need to determine the translation and rotation for each gaussian from the neutral pose to the target frame pose.
% For the translation, our goal is to therefore determine the position of each gaussian in the target frame pose:
  % 1. Take the query points defined in the neutral canonical space and add the learned offset_xyz
  % 2. We then map these query points from the neutral canonical pose to the zero pose where we add the blendshapes offsets to account for spefic body shapes
  % 3. Finally, given the precomputed skinning weights and the smplx pose parameters, we map the query points from the zero pose to the target posed space
% As a result, we obtain the xyz position of each gaussian in the target frame pose

%  For the rotation, we do the following:
  % 1. Extract the rotation matrix that maps from the neutral canonical pose to the target posed space for each query point
  % 2. Convert this rotation matrix to quaternion representation and normalize it to ensure valid rotation
  % 3. We then apply then defined this rotation to a subset of 3dgs that are constrained to the body (e.g., torso, arms, legs) to avoid unwanted rotation artifacts, 
  % for the rest of the 3dgs we keep the rotation as in the canonical spacea
% As a result, we obtain rotatatio from the neutral pose to the target frame pose for each gaussian in the form of quaternions

% We then render the posed 3dgs into images using differentiable rendering via gsplat library.

% Benefits of my design choices:
% 1. the 3DGS is free to learn the offset from the SMPLX surface, so in theory it can learn clothing dynamics and other non-rigid deformations that are not captured by the SMPLX model itself
% 2. i should explain why do we actually have "special" neutral space - not just because that's what LHM does...

We use a human-centric dynamic scene representation that combines a parametric body model (SMPL-X) with 3D Gaussian Splatting (3DGS). Throughout this section, we distinguish three poses. These are (i) the \emph{zero pose}, where all SMPL-X pose parameters are set to zero, (ii) a fixed \emph{neutral pose} (an A-pose-like configuration used to define our canonical space), and (iii) the \emph{target frame pose}, which denotes the estimated SMPL-X parameters for a particular video frame. In the following, we describe how we define our canonical 3DGS in the neutral pose and how we deform it to the target frame pose using SMPL-X.

\paragraph{Neutrally posed query points and skinning weights.}
We start by constructing a dense set of $N$ query points on the SMPL-X surface in the neutral pose (in our experiments, $N=40{,}000$). Concretely, we use a precomputed dense point set and assign a body/face split by a fixed ratio. We use ratio $3$ that yields approximately $30{,}000$ body points and $10{,}000$ face points. To deform these points with SMPL-X, we require linear blend skinning (LBS) weights. For each dense point, we find its nearest neighbor on the SMPL-X template mesh and copy the corresponding template vertex LBS weights, resulting in a per-point weight matrix of shape $N\times 55$ (55 SMPL-X joints in our setup). Finally, we compute and store a per-joint transformation that maps between the neutral-pose space and the SMPL-X zero-pose space, represented as a matrix of shape $(55,4,4)$. This transform is later converted into per-point transforms via the LBS weights.

\paragraph{Canonical 3DGS per person.}
For each person, we represent appearance and geometry in the neutral pose using a set of $N$ Gaussians. Each Gaussian stores its center offset relative to the neutral query point ($\Delta\mathbf{x}\in\mathbb{R}^{N\times 3}$), a rotation quaternion ($\mathbf{q}\in\mathbb{R}^{N\times 4}$), anisotropic scaling ($\mathbf{s}\in\mathbb{R}^{N\times 3}$), opacity ($\alpha\in\mathbb{R}^{N\times 1}$), and per-Gaussian color (RGB in our current implementation). Intuitively, anchoring Gaussians to the SMPL-X surface provides a strong prior for multi-person scenes, while the learned offsets allow the representation to deviate from the parametric surface (e.g., to capture clothing and other non-rigid details).

\paragraph{Deforming Gaussians to a target frame.}
Given SMPL-X parameters for a target frame, we deform the canonical Gaussians from the neutral pose to the target posed space. For the translation, we first form canonical Gaussian centers by adding the learned offsets to the neutral query points. We then map these centers from the neutral pose to the SMPL-X zero pose using the precomputed neutral-to-zero transforms (blended per point with LBS weights). In the zero-pose space, we apply SMPL-X blendshape offsets (from the subject-specific shape parameters) and finally warp the points from the zero pose to the target pose using standard SMPL-X forward kinematics and LBS. This produces the posed Gaussian centers $\mathbf{x}_t$ for the frame.

For the rotation, we extract the per-point rigid rotation component of the neutral-to-posed transform and convert it to unit quaternions. We then compose this rotation with each Gaussian's canonical quaternion to obtain the posed Gaussian orientations. In addition, for a subset of Gaussians associated with constrained body regions, we optionally suppress the additional rotation to avoid undesirable orientation artifacts.


\section{Training objectives}
\label{sec:training-objectives}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/train_step_example.drawio.png}
    \caption{\textbf{Overview of a single training step.} Overview of a single training step going from the canonical 3DGS and estimated SMPL-X and camera parameters to the rendered images and loss computation. We also show the gradient flow during pose and 3DGS optimisation phase.}
    \label{fig:trn_step_example}  
\end{figure}

In this section, we present our two-stage training procedure (pose tuning followed by 3DGS optimization) and define the loss terms used in each stage.


\paragraph{Training overview.} Figure~\ref{fig:trn_step_example} shows an example training step. At each iteration, we sample a mini-batch of frames. Given the camera parameters and the per-frame SMPL-X parameters, we deform the canonical 3DGS to the posed space for each sampled frame and render the corresponding RGB image, alpha mask, and depth using the differentiable renderer from gsplat \cite{ye2025gsplat}. We then apply several loss terms to supervise the rendered images against the corresponding pseudo ground-truth signals (see below). Importantly, we divide our training into two stages: (i) a \emph{pose optimization} stage, where we optimize only the SMPL-X parameters while keeping the 3DGS fixed, and (ii) a \emph{3DGS optimization} stage, where we optimize only the 3DGS parameters while keeping the SMPL-X parameters fixed. After the pose optimization stage, we use the updated pose parameters and the LHM-initialized 3DGS to synthesize novel training views using DiFix, as described in the previous section. We then proceed to the 3DGS optimization stage using the synthesized novel views as additional supervision. During this phase, each batch can contain frames from either the source camera or one of the synthesized novel training cameras.

\paragraph{Photometric loss.}
We supervise the rendered RGB using foreground masks to focus the objective on the humans and reduce the influence of the static background. For a mini-batch $\mathcal{B}$, let $\hat{\mathbf{I}}_b\in[0,1]^{H\times W\times 3}$ denote the rendered RGB image, $\mathbf{I}_b$ the ground-truth RGB, and $\mathbf{M}_b\in[0,1]^{H\times W}$ the foreground mask (broadcast to three channels as $\mathbf{M}_b^3$). We form the masked target $\mathbf{I}^{m}_b=\mathbf{I}_b\odot\mathbf{M}_b^3$ and compute a masked RGB reconstruction loss term as
\begin{equation}
  \mathcal{L}_{\mathrm{rgb}} = \frac{1}{|\mathcal{B}|}\sum_{b\in\mathcal{B}} \mathrm{MSE}\!\left(\hat{\mathbf{I}}_b, \mathbf{I}^{m}_b\right),
\end{equation}
and an SSIM-based structural term on the same masked images
\begin{equation}
  \mathcal{L}_{\mathrm{ssim}} = \frac{1}{|\mathcal{B}|}\sum_{b\in\mathcal{B}}\left(1 - \mathrm{SSIM}\!\left(\hat{\mathbf{I}}_b, \mathbf{I}^{m}_b\right)\right).
\end{equation}
Here, $\mathrm{MSE}(\mathbf{A},\mathbf{B})=\lVert \mathbf{A}-\mathbf{B}\rVert_2^2/|\mathbf{A}|$ denotes the mean squared error over all elements. In our implementation, SSIM is computed with \texttt{valid} padding, i.e., only over pixels where the full SSIM window fits inside the image.

\paragraph{Silhouette loss.}
We apply an additional silhouette loss on the predicted alpha mask, but only for samples coming from the source camera. This choice avoids strong gradients from potentially noisy masks in novel views and keeps the silhouette supervision anchored to the most reliable segmentation. For a mini-batch $\mathcal{B}$, let $\mathcal{S}\subseteq\mathcal{B}$ be the subset of indices corresponding to source-view samples, and let $\hat{\mathbf{A}}_b\in[0,1]^{H\times W}$ and $\mathbf{M}_b\in[0,1]^{H\times W}$ denote the rendered alpha mask and the corresponding estimated foreground mask. We compute
\begin{equation}
  \mathcal{L}_{\mathrm{sil}} =
  \begin{cases}
    \frac{1}{|\mathcal{S}|}\sum_{b\in\mathcal{S}} \mathrm{MSE}\!\left(\hat{\mathbf{A}}_b, \mathbf{M}_b\right), & |\mathcal{S}|>0,\\
    0, & |\mathcal{S}|=0,
  \end{cases}
\end{equation}
where $|\mathcal{S}|$ is the number of source-view samples in the mini-batch.

\paragraph{Depth loss.}
To also inject 3D prior knowledge, we apply an additional depth loss on the rendered depth map, but only for samples coming from the source camera. This is because we found that Depth Anything 3 fails when predicting depth for images with little to no background, which is the case for our synthesized novel training views. For a mini-batch $\mathcal{B}$ and its source-view subset $\mathcal{S}\subseteq\mathcal{B}$, let $\hat{\mathbf{D}}_b\in\mathbb{R}^{H\times W}$ denote the rendered depth and $\mathbf{D}_b$ the corresponding pseudo ground-truth depth estimate. We mask depth using the foreground mask $\mathbf{M}_b$ as $\mathbf{D}^m_b=\mathbf{D}_b\odot \mathbf{M}_b$ and compute
\begin{equation}
  \mathcal{L}_{\mathrm{depth}} =
  \begin{cases}
    \frac{1}{|\mathcal{S}|}\sum_{b\in\mathcal{S}} \mathrm{MSE}\!\left(\hat{\mathbf{D}}_b, \mathbf{D}^{m}_b\right), & |\mathcal{S}|>0,\\
    0, & |\mathcal{S}|=0,
  \end{cases}
\end{equation}
where $|\mathcal{S}|$ is the number of source-view samples in the mini-batch.

\paragraph{Canonical regularization.}
We regularize the canonical 3DGS using two terms adopted from LHM \cite{qiu2025lhm}. The ASAP term (as spherical as possible) encourages isotropic Gaussian scales, which empirically reduces blurry edges. The ACAP term (as close as possible) encourages Gaussians to stay close to the SMPL-X surface and prevents drifting artifacts \cite{qiu2025lhm}.\newline
For ASAP, let $\mathbf{s}\in\mathbb{R}^{N\times 3}$ denote the (log-)scales and $\mathbf{s}^+=\exp(\mathbf{s})$ their positive version, we compute
\begin{equation}
  \mathcal{L}_{\mathrm{ASAP}} = \frac{1}{N}\sum_{i=1}^{N}\left\lVert \mathbf{s}^+_i - \mathrm{mean}(\mathbf{s}^+_i)\mathbf{1}\right\rVert_2^2.
\end{equation}
For ACAP, we penalize large offset magnitudes beyond a margin $m$ (we use $m=0.03$m), where $\Delta\mathbf{x}_i$ denotes the learned offset of Gaussian $i$:
\begin{equation}
  \mathcal{L}_{\mathrm{ACAP}} = \frac{1}{N}\sum_{i=1}^{N}\max\left(0, \lVert \Delta\mathbf{x}_i \rVert_2 - m\right).
\end{equation}
Putting everything together, our total objective is a weighted sum of the above terms.
\begin{equation}
  \mathcal{L} =
  w_{\mathrm{rgb}}\mathcal{L}_{\mathrm{rgb}} +
  w_{\mathrm{ssim}}\mathcal{L}_{\mathrm{ssim}} +
  w_{\mathrm{sil}}\mathcal{L}_{\mathrm{sil}} +
  w_{\mathrm{depth}}\mathcal{L}_{\mathrm{depth}} +
  w_{\mathrm{asap}}\mathcal{L}_{\mathrm{ASAP}} +
  w_{\mathrm{acap}}\mathcal{L}_{\mathrm{ACAP}}.
\end{equation}
Empirically, we found the following weights work best $w_{\mathrm{rgb}}=20.0$, $w_{\mathrm{ssim}}=0.8$, $w_{\mathrm{sil}}=2.0$, $w_{\mathrm{depth}}=0.5$, $w_{\mathrm{asap}}=200.0$, and $w_{\mathrm{acap}}=200.0$.
