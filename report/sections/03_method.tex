%% ----------------------------------------------------------------------------
% CVG SA/MA thesis template
%
% Created 03/08/2024 by Tobias Fischer
%% ----------------------------------------------------------------------------

% -------- Instructions for writing the method section:
% The objectives of the Method section are the following: 
% \begin{itemize}
 
 % \item \textit{What problem are you solving?} Give a clear, formalized statement of the problem you are solving, \ie which inputs are given, which assumptions you make, and what is the desired output (goal).
 % \item \textit{What are the preliminaries of your work?} Introduce the environment in which your work has taken place - this can be a software package, a device, or a system description. Ensure sufficiently detailed descriptions of the algorithms and concepts (e.g. math) you used.
 % \item \textit{What is your work?} Describe the key component of your work, e.g. an algorithm or software framework you have developed.
% \end{itemize}


\newpage
\chapter{Method}

% todo: maybe explain briefly what are homogeneous coordinates
\section{Preliminaries}
\subsection{Homogeneous coordinate system}

In 3D computer vision and graphics we frequently apply rigid and affine transformations to points and want to compose multiple transformations efficiently. Homogeneous coordinates unify linear and affine transformations by representing a 3D point $\mathbf{p}=[x,y,z]^{T}$ as a 4D vector $\mathbf{p}_{h}=[x,y,z,1]^{T}$, so that transformations can be written as $4\times 4$ matrices and composed by matrix multiplication. A general rigid transformation (an element of SE(3)) can be written as:

\begin{equation}
A = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix}
\end{equation}

where $R$ is a $3\times 3$ rotation matrix and $t$ is a $3\times 1$ translation vector. We make use of homogeneous coordinates throughout this work, especially when working with SMPL-X transformations and 3D Gaussian Splatting.

\subsection{SMPLX Body Model}
Skinned Multi-Person Linear Model eXtended (SMPL-X) \cite{smplx} is a parametric 3D human body model that represents a minimally clothed human body (it does not model clothing geometry). It defines parameters that control both body shape and articulation. Shape parameters $\beta$ capture person-specific attributes such as body proportions and are constant across frames. Pose parameters $\theta$ control the articulation of the body (and, in SMPL-X, also hands and face). Given these parameters, SMPL-X maps a canonical template mesh to a posed mesh using forward kinematics and linear blend skinning (LBS). In our work, we use SMPL-X as a strong geometric prior to position and deform our explicit 3D Gaussian Splatting representation. Unless stated otherwise, SMPL-X parameters are estimated during preprocessing and are kept fixed during the 3DGS optimization stage.

\paragraph{SMPLX model overview.} In general, SMPLX is a mesh based model defined as the following function:

\begin{equation}
M(\beta, \theta, \psi, t) \rightarrow V \in \mathbb{R}^{N \times 3}
\end{equation}

where $\beta$ are the shape parameters, $\theta$ are the pose parameters, $\psi$ are the facial expression parameters, $t$ is the global translation of the body in the world coordinate system, and $V$ are the output mesh vertices of the body with $N$ vertices \cite{smplx}.

\paragraph{Applying shape parameters.} We start from the template mesh in the zero T-Pose defined as 
$T_{0} \in \mathbb{R}^{N \times 3}$. We then apply the shape parameters $\beta$ using blendshapes as follows:

\begin{equation}
T_{shaped} = T_{0} + B_{s}(\beta)
\end{equation}

% todo: here we need to explain what is B_s function
where $B_{s}(\beta)$ is the linear blendshape function that takes as input the shape parameters $\beta$ and outputs the per-vertex offsets to be applied to the template mesh. Optionally, we can also apply expression blendshapes $B_{e}(\psi)$ to account for facial expressions:

\begin{equation}
T_{shaped,expr} = T_{0} + B_{s}(\beta) + B_{e}(\psi)
\end{equation}

% todo: here we should mention that the reason why we are not using expression blendshapes is because human3r does not estimate them reliably
In practice, in our work, we do not use facial expressions, so we only apply the shape blendshapes.

% todo: cite rodriques formula paper
\paragraph{Converting pose parameters into global transformations.} Next, we convert the pose parameters $\theta$ from the axis-angle representation to rotation matrices using Rodrigues' formula. As a result, for each of the $K$ joints in the kinematic tree of the SMPLX body model, we obtain a rotation matrix $R_{i} \in \mathbb{R}^{3 \times 3}$ where $i = 1, \ldots, K$. These rotation matrices for each joint represent the local rotation of the joint with respect to its parent joint in the kinematic tree. 

\paragraph{Pose-dependent deformations.}
In addition to shape and expression blendshapes, the standard SMPL-X formulation includes pose-dependent blendshapes $B_{p}(\theta)$, which model non-linear pose effects such as muscle bulging. We rely on the standard SMPL-X implementation \cite{smplx} for these details.

We convert each of these local joint \textit{rotations} to local joint \textit{transformation} matrices $G_{i} \in \mathbb{R}^{4 \times 4}$ by appending the joint translation and a row for homogeneous coordinates as follows:

\begin{equation}
G_{i} = \begin{bmatrix} R_{i} & t_{i} \\ 0 & 1 \end{bmatrix}
\end{equation}

where $t_{i} \in \mathbb{R}^{3}$ is the translation vector of joint $i$ with respect to its parent joint defined precisely as follows:

\begin{equation}
t_{i} = J_{i}(\beta) - J_{parent(i)}(\beta)
\end{equation}

% todo: maybe complement this mathy explanation also with one sentence explanation on a more intuitive level
where $J_{i}(\beta)$ is the 3D position of joint $i$ given the shape parameters $\beta$, and $parent(i)$ returns the index of the parent joint of joint $i$ in the kinematic tree. 

Finally, to transform the rigid transformations from the local joint space to the world coordinate system, we perform forward kinematics as follows:

\begin{equation}
G_{i}^{world} = G_{parent(i)}^{world} \cdot G_{i}
\end{equation}

where $G_{i}^{world}$ is the world transformation of joint $i$, and $G_{parent(i)}^{world}$ is the world transformation of the parent joint of joint $i$. We perform this operation recursively starting from the root joint which has no parent.

% todo: we should explain are the skinning weights estimated 
\paragraph{Posing the mesh via Linear Blend skinning and global pose transformations.} Finally, we pose the mesh vertices using Linear Blend Skinning (LBS) and the global joint transformations obtained from forward kinematics. Each vertex $v_{j}$ in the mesh is associated with a set of skinning weights $w_{j,i}$ for each joint $i$. The posed vertex position $v_{j}^{posed}$ is then computed as follows:

\begin{equation}
v_{j}^{posed} = \sum_{i=1}^{K} w_{j,i} \cdot \left(G_{i}^{world} \cdot \begin{bmatrix} v_{j}^{shaped} + B_{p}(\theta)_{j} \\ 1 \end{bmatrix}\right)_{1:3}
\end{equation}

where $v_{j}^{shaped}$ is the vertex position after applying shape blendshapes, $B_{p}(\theta)_{j}$ is the pose-dependent offset for vertex $j$, and $(\cdot)_{1:3}$ extracts the first three components of the resulting homogeneous coordinate. This operation is performed for all vertices in the mesh to obtain the final posed mesh $V^{posed}$.

\paragraph{Adding global translation.} To complete the posing process, we add the global translation $t$ to all posed vertices as follows:
\begin{equation}
V^{final} = V^{posed} + t
\end{equation}

where $V^{final}$ is the final posed mesh with global translation applied.


\subsection{3D Gaussians}
% - representation of 3D scene using 3D Gaussians
% - parameters: position, covariance, color, opacity, etc.
% - volume rendering with 3D Gaussians and camera parameters


\section{Problem Definition}
% - Input and output specification
% Input: unposed monocular video of dynamic human centric scene, captured from either static or moving camera
% Output: 4D representation of the scene: a. canonical 3dgs b. smplx parameters over time c. camera parameters over time
% d. if needed 3dgs can be converted to mesh representation for easier visualisation. Everything is meters and in world coordinate system.

% - Assumptions
% - we currently do not account for blurry images - so we assume that the input video is of decent quality with minimal motion blur

% - Scope and evaluation goals
% - Our goal is to obtain high rendering quality for novel view synthesis, as well as accurate recovery of human motion in the scene. We evaluate our method on both quantitative metrics such as PSNR, SSIM, LPIPS for novel view synthesis
% - We also want to obtain high pose quality which we measure using MPJPE, MVE, Contact Distance and Percentage of Correct Depth relations
% - As another proxy for quality of extracted geometry, we also measure mesh reconstrution quality using V-IoU, Chamfer Distance, P2S and Normal Consistency
% - Finally, we also evaluate the segmentation quality using IoU, F1 and Recall metrics - segmentation is important for being able to distenghuish dynamic and static parts of the scene

Our input consists only of the RGB frames of a monocular video capturing multiple people interacting in a dynamic scene. The video can be captured with either a static or a moving camera. No camera parameters, segmentations, or body model parameters are given at test time. Our pipeline estimates, for each frame, the camera parameters and per-person SMPL-X parameters, and uses them to optimize a canonical 3D Gaussian Splatting (3DGS) representation. The final output is a 4D scene representation that includes (i) a canonical 3DGS model, (ii) per-frame SMPL-X parameters for each person, and (iii) per-frame camera parameters. If needed, the 3DGS can be converted to a mesh representation for visualization and evaluation. We represent geometry in meters.

We assume that the input video does not contain severe motion blur and that the people are not persistently occluded by large objects. We focus on reconstructing the dynamic humans and do not aim to reconstruct dynamic non-human objects. Our experiments cover short sequences (up to about 10 seconds), and we treat longer videos as outside the validated scope of this work.

Our evaluation axes reflect the overall objectives of the thesis. First, we target high-quality appearance from arbitrary viewpoints, which we assess via novel view synthesis. Second, we aim for realistic geometry and motion, which we assess via pose estimation and mesh reconstruction metrics. Third, we aim to correctly separate dynamic humans from the static background, which we assess via instance segmentation metrics. We define the specific metrics and evaluation protocols in the Experiments section.


\section{Preprocessing}

Our pipeline relies on a preprocessing stage that estimates camera and human priors from the raw monocular video. All three components are necessary: Human3R provides per-frame cameras and SMPL-X parameters, SAM3 provides instance masks, and LHM provides a per-person canonical 3DGS initialization. This initialization is later crucial for novel training view synthesis, where we render posed 3DGS and refine them with DiFix. If the initial renderings are poor, the image-to-image refinement becomes substantially harder.

\paragraph{Human3R.}
We first estimate SMPL-X parameters and camera parameters for every frame using Human3R \cite{chen2025human3r}. Human3R outputs all estimates in a shared world coordinate frame and in metric units.

\paragraph{SAM3.}
Next, we run SAM3 \cite{carion2025sam3segmentconcepts} to obtain instance segmentation masks for each person. We prompt the video model with ``a person'' and rely on its temporal tracking to maintain consistent identities across frames.

\paragraph{LHM initialization.}
Finally, we initialize a canonical 3DGS model for each person using LHM \cite{qiu2025lhm}. LHM predicts the Gaussians in its neutral canonical pose space. For simplicity, we always use the first video frame as the reference frame: we mask the frame with the corresponding SAM3 instance mask and feed the masked image to LHM.

\paragraph{Limitations.}
Preprocessing can fail in several ways. SAM3 may miss all people in a scene or lose a person for a subset of frames, which requires manual inspection. In addition, mask track identities may not always match the motion tracks and ground truth track identities, and we need to ensure that we process all detected people. These limitations motivate careful curation of the preprocessed outputs before training.


\section{Novel Training View Synthesis with DiFix}

% inclue this figure here:
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/novel_view_gen_overview.drawio.png}
    \caption{Overview of our novel training view synthesis pipeline using DiFix. We start with the monocular video and the estimated SMPLX parameters to render initial novel views of the humans using the canonical 3DGS obtained from LHM during preprocessing. These initial renderings are then refined using DiFix to ensure multi-view and temporal consistency. The refined novel views are then used as input to our main training pipeline.}
    \label{fig:novel_view_synthesis_difix}  
\end{figure}


% explain how DiFix works
Our explicit scene representation is sensitive to the number of input views, and easily overfits to the training views, while for unseen views, we get blurry artifacts and holes in the reconstruction. For this reason, it is crucial to lift the monocular video to a set of multi-view videos to provide multi-view guidance during trainig and avoid the before mentioned issues.

Synthesising novel training videos from monocular video is indeed highly ill posed problem. The key challenge is to produce novel videos that are both multi-view as well as temporally consistent. Therefore, to make the problem simpler, we make use of the following two insights. First, it is much simpler to synthesie novel views which are close to the original view. In other words, we can recursively synthesie novel views that are slightly offset from the original view, and then use these novel views as input to synthesie further novel views. Second, it is much easier to synthesie apperance of the person if we already have a initial good estimate. For this reason, we make use of the canonical 3DGS obtained from LHM during preprocessing to render initial novel views of the person, and then use these renderings as input to DiFix \cite{wu2025difix3d} to refine the renderings and ensure multi-view and temporal consistency.


% explain the algorithm for synthesising novel training views
Figure \ref{fig:novel_view_synthesis_difix} shows an overview of our algorithm for obtaining novel view videos based on the source monocular video. First, DiFix is image-to-image diffusion model (finetuned version of Stable Diffusion) that expects three inputs: a. reference image b. image to refine c. prompt.
We use the default prompt from the original paper which is "remove degradation". The first row of Figure \ref{fig:novel_view_synthesis_difix} shows how we obtain novel views which are progressively farther from the original view. For instance, in the first step, our goal is to synthesise novel views for cameras with IDs 16 and 88 (hence orange color). We therefore use as the reference view the source camera with the ID 4 (blue color). The second row of Figure \ref{fig:novel_view_synthesis_difix} shows the actual process of obtaining the novel views for selected camera frame by frame. For instance, to synthesise novel view for camera 88 whose reference camera is camera 4, we follow these steps to synthesise the novel view at frame $t$:

\begin{enumerate}
  \item Sample frame $t$ from the reference camera 4
  \item Render frame $t$ from target camera 88 using the canonical 3DGS obtained from LHM during preprocessing. This gives us an initial estimate of the novel view.
  \item Feed both images to DiFix to obtain the refined novel view for camera 88 at frame $t$.
\end{enumerate}

We repeat the above process for all frames in the video sequence to obtain the full novel view video for camera 88. We repeat the entire process for all target cameras to obtain a set of novel view videos that are then used as input to our main training pipeline. Figure \ref{fig:trn_nv_cam_gen_3d_vis} shows a 3D visualisation of the generated novel training view cameras around the original scene. We generate this circle of novel training cameras before starting the DiFix synthesis process. Cameras are generated in such a way that they are evenly distribured in a circle around the scene such that across all frames, the estimated SMPLX humans are always in the field of view of all cameras.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/trn_view_cam_gen_in_3d.png}
    \caption{Visualisation of the generated generated training view cameras around the original scene. The generated cameras are in orange, while the original camera trajectory is in blue. We use Viser \cite{yi2025viser} to obtain this visualisation.}
    \label{fig:trn_nv_cam_gen_3d_vis}  
\end{figure}


\section{3DGS-based Dynamic Representation}

% Terminology definitions:
% 1. Neutral pose - our own pose definition which is similar to A pose (have to double check this)
% 2. Zero pose - SMPLX zero pose definition (all pose parameters set to zero)
% 3. Target frame pose - estimated SMPLX pose for a given frame in the video sequence
% 4. N = number of 3dgs vertices after upsampling = 40k

% We define the neutrally posed upsampled mesh vertex as follows:
% 1. We first define a neutral pose for the SMPLX model - this is similar to A-pose
% 2. We then obtain the neutral pose mesh vertices - approximately 10k vertices
% 3. We upsample these vertices to 40k vertices using the body/face ratio: 3 -> we therefore end up with around 30k body vertices and 10k face vertices
% 4. For each upsampled vertex, we also obtain the corresponding LBS weights for all 55 joints by finding the closest original vertex from the original 10k vertices -> we get 40k x 55 LBS weights matrix
% In addition, we store the transformation that takes us from the neutral pose back to the zero -> matrix of shape (55, 4, 4)


% LHM then predicts the canonical 3DGS parameters in the neutral pose space:
% 1. mean offsets from the neutral pose vertices, shape Nx3
% 2. rotation represented as quarternions, shape Nx4
% 3. scales represented as diagonal covariance matrices, with shape Nx3, therefore each of the 3 dims defines the variance along the corresponding axis
% 4. opacity, shape Nx1
% 5. colors in RGB space, shape Nx3

% Deformation from the neutral pose to the target frame pose then works as follows.
% On a high level, we need to determine the translation and rotation for each gaussian from the neutral pose to the target frame pose.
% For the translation, our goal is to therefore determine the position of each gaussian in the target frame pose:
  % 1. Take the query points defined in the neutral canonical space and add the learned offset_xyz
  % 2. We then map these query points from the neutral canonical pose to the zero pose where we add the blendshapes offsets to account for spefic body shapes
  % 3. Finally, given the precomputed skinning weights and the smplx pose parameters, we map the query points from the zero pose to the target posed space
% As a result, we obtain the xyz position of each gaussian in the target frame pose

%  For the rotation, we do the following:
  % 1. Extract the rotation matrix that maps from the neutral canonical pose to the target posed space for each query point
  % 2. Convert this rotation matrix to quaternion representation and normalize it to ensure valid rotation
  % 3. We then apply then defined this rotation to a subset of 3dgs that are constrained to the body (e.g., torso, arms, legs) to avoid unwanted rotation artifacts, 
  % for the rest of the 3dgs we keep the rotation as in the canonical spacea
% As a result, we obtain rotatatio from the neutral pose to the target frame pose for each gaussian in the form of quaternions

% We then render the posed 3dgs into images using differentiable rendering via gsplat library.

% Benefits of my design choices:
% 1. the 3DGS is free to learn the offset from the SMPLX surface, so in theory it can learn clothing dynamics and other non-rigid deformations that are not captured by the SMPLX model itself
% 2. i should explain why do we actually have "special" neutral space - not just because that's what LHM does...

We use a human-centric dynamic scene representation that combines a parametric body model (SMPL-X) with 3D Gaussian Splatting (3DGS). Throughout this section, we distinguish three poses: (i) the \emph{zero pose}, where all SMPL-X pose parameters are set to zero; (ii) a fixed \emph{neutral pose} (an A-pose-like configuration used to define our canonical space); and (iii) the \emph{target frame pose}, i.e., the estimated SMPL-X parameters for a particular video frame.

\paragraph{Neutrally posed query points and skinning weights.}
We start by constructing a dense set of $N$ query points on the SMPL-X surface in the neutral pose (in our experiments, $N=40{,}000$). Concretely, we use a precomputed dense point set and assign a body/face split by a fixed ratio (e.g., ratio $3$ yields approximately $30{,}000$ body points and $10{,}000$ face points). To deform these points with SMPL-X, we require linear blend skinning (LBS) weights. For each dense point, we find its nearest neighbor on the SMPL-X template mesh and copy the corresponding template vertex LBS weights, resulting in a per-point weight matrix of shape $N\times 55$ (55 SMPL-X joints in our setup). Finally, we compute and store a per-joint transformation that maps between the neutral-pose space and the SMPL-X zero-pose space, represented as a matrix of shape $(55,4,4)$; this transform is later converted into per-point transforms via the LBS weights.

\paragraph{Canonical 3DGS per person.}
For each person, we represent appearance and geometry in the neutral pose using a set of $N$ Gaussians. Each Gaussian stores its center offset relative to the neutral query point ($\Delta\mathbf{x}\in\mathbb{R}^{N\times 3}$), a rotation quaternion ($\mathbf{q}\in\mathbb{R}^{N\times 4}$), anisotropic scaling ($\mathbf{s}\in\mathbb{R}^{N\times 3}$), opacity ($\alpha\in\mathbb{R}^{N\times 1}$), and per-Gaussian color (RGB in our current implementation). Intuitively, anchoring Gaussians to the SMPL-X surface provides a strong prior for multi-person scenes, while the learned offsets allow the representation to deviate from the parametric surface (e.g., to capture clothing and other non-rigid details).

\paragraph{Deforming Gaussians to a target frame.}
Given SMPL-X parameters for a target frame, we deform the canonical Gaussians from the neutral pose to the target posed space. For the translation, we first form canonical Gaussian centers by adding the learned offsets to the neutral query points. We then map these centers from the neutral pose to the SMPL-X zero pose using the precomputed neutral-to-zero transforms (blended per point with LBS weights). In the zero-pose space, we apply SMPL-X blendshape offsets (from the subject-specific shape parameters) and finally warp the points from the zero pose to the target pose using standard SMPL-X forward kinematics and LBS. This produces the posed Gaussian centers $\mathbf{x}_t$ for the frame.

For the rotation, we extract the per-point rigid rotation component of the neutral-to-posed transform and convert it to unit quaternions. We then compose this rotation with each Gaussian's canonical quaternion to obtain the posed Gaussian orientations. In addition, for a subset of Gaussians associated with constrained body regions, we optionally suppress the additional rotation to avoid undesirable orientation artifacts.

\paragraph{Rendering.}
For each view, we deform each person independently, concatenate all posed Gaussians in the scene, and render RGB, mask, and depth using a differentiable 3DGS renderer (gsplat).

\section{Training objectives}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/trn_pip_overview.drawio.png}
    \caption{Overview of our training pipeline.}
    \label{fig:trn_pip_overview}  
\end{figure}

% High level process:
% 1. We sample a batch of frames from different views (source camera + generated novel views)
% 2. Using the view camera parameters and the per-frame SMPLX parameters, we deform the canonical 3DGS to the posed space 
%   for each frame in the batch and render the images using differentiable rendering via gsplat library.
% 3. We then compute the losses between the rendered images and the ground truth images using the foreground masks to focus only on the humans in the scene.
% We have the following losses:
  % a. RGB reconstruction loss (masked MSE + SSIM)
  % b. Silhouette loss (only on source camera view since this is where we have the most reliable masks)
  % c. Canonical regularization losses (ASAP/ACAP) to encourage plausible geometry in the canonical space, formulas:
  % 1. scales_pos = torch.exp(scales)
  %    asap = ((scales_pos - scales_pos.mean(dim=-1, keepdim=True)) ** 2).sum(dim=-1).mean()
  % 2. acap = torch.clamp(offsets.norm(dim=-1) - self.cfg.regularization['acap_margin'], min=0.0).mean() where we use by default acap_margin = 0.01 (1cm)


% things to improve here:
% 1. use paragraph stle to split the these explanation into: photometric loss, silhouette loss, canonical regularization losses
% 2. At the end, we should mention the full objectie and mention that empirically we found the following weights to work the best for each loss term: 
% loss_weights:
  % rgb: 20.0
  % sil: 4.0
  % ssim: 0.4
  % depth: 0.0 # for now disabled because I have unreliable depth maps for nvs

% regularization:
  % asap_w: 200.0
  % acap_w: 200.0
  % acap_margin: 0.01 # meters (1 cm)

% 3. we need to acknowledge that the regularisaton terms are adopted from the lhm paper, therefore use
% \cite{qiu2025lhm} to cite them. The ASAP stands for as spheric as possible, and ACAP stands for as close as possible (both of these terms were introduced in LHM paper. We should paraphrase the explanations from LHM paper about these terms and logic behind them:
% a. ASAP term is to encourage isotropic scales for each gaussian - in the lhm paper, they found that this term removed weird blurry edges in the reconstruction
% b. ACAP term is to encourage the gaussians to stay close to the smplx surface - in the lhm paper, they found that this term prevented gaussians from drifting too far away from the smplx surface which would lead to artifacts in the reconstruction 


Our training objective follows the pipeline shown in Figure \ref{fig:trn_pip_overview}. At each iteration, we sample a mini-batch of frames from a mixture of views (the source camera and the synthesized novel training views). Given the camera parameters and the per-frame SMPL-X parameters, we deform the canonical 3DGS to the posed space for each sampled frame and render the corresponding RGB image (and auxiliary outputs) using differentiable Gaussian splatting.

\paragraph{Photometric loss.}
We supervise the rendered RGB with foreground masks to focus the objective on the humans and reduce the influence of the static background. Our photometric objective combines a masked RGB reconstruction loss (mean squared error) with a perceptual structural term based on SSIM computed on the masked images.

\paragraph{Silhouette loss.}
We apply an additional silhouette loss on the predicted alpha mask, but only for samples coming from the source camera. This choice avoids strong gradients from potentially noisy masks in novel views and keeps the silhouette supervision anchored to the most reliable segmentation.

\paragraph{Canonical regularization.}
We regularize the canonical 3DGS using two terms adopted from LHM \cite{qiu2025lhm}. The ASAP term (as spherical as possible) encourages isotropic Gaussian scales, which empirically reduces blurry edges. The ACAP term (as close as possible) encourages Gaussians to stay close to the SMPL-X surface and prevents drifting artifacts.

For ASAP, let $\mathbf{s}\in\mathbb{R}^{N\times 3}$ denote the (log-)scales and $\mathbf{s}^+=\exp(\mathbf{s})$ their positive version; we compute
\begin{equation}
  \mathcal{L}_{\mathrm{ASAP}} = \frac{1}{N}\sum_{i=1}^{N}\left\lVert \mathbf{s}^+_i - \mathrm{mean}(\mathbf{s}^+_i)\mathbf{1}\right\rVert_2^2.
\end{equation}
For ACAP, we penalize large offset magnitudes beyond a margin $m$ (we use $m=0.01$m), where $\Delta\mathbf{x}_i$ denotes the learned offset of Gaussian $i$:
\begin{equation}
  \mathcal{L}_{\mathrm{ACAP}} = \frac{1}{N}\sum_{i=1}^{N}\max\left(0, \lVert \Delta\mathbf{x}_i \rVert_2 - m\right).
\end{equation}

Putting everything together, our total objective is a weighted sum of the above terms:
\begin{equation}
  \mathcal{L} =
  w_{\mathrm{rgb}}\mathcal{L}_{\mathrm{rgb}} +
  w_{\mathrm{ssim}}\mathcal{L}_{\mathrm{ssim}} +
  w_{\mathrm{sil}}\mathcal{L}_{\mathrm{sil}} +
  w_{\mathrm{asap}}\mathcal{L}_{\mathrm{ASAP}} +
  w_{\mathrm{acap}}\mathcal{L}_{\mathrm{ACAP}}.
\end{equation}
Empirically, we found the following weights work best $w_{\mathrm{rgb}}=20.0$, $w_{\mathrm{ssim}}=0.4$, $w_{\mathrm{sil}}=4.0$, $w_{\mathrm{asap}}=200.0$, and $w_{\mathrm{acap}}=200.0$.
